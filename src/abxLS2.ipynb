{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "assert platform.node() == 'habilis', \"Code should run on habilis\"\n",
    "\"\"\" Global Paths \"\"\"\n",
    "source_leaderboards = Path('/home/nhamilakis/workspace/zerospeech/core/leaderboards/data/tasks/abx/abxLS/parted')\n",
    "new_scores_location = Path('/scratch2/mhallap/zerospeech/evaluations/abx_evaluations/2022_evals/dtw_runs/v095/results')\n",
    "dest_dir = Path('/home/nhamilakis/workspace/zerospeech/core/leaderboards/data/tasks/abx/abxLS-v2/parted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Old Leaderboard definitions \"\"\"\n",
    "from typing import Literal\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class ABXLSScore:\n",
    "    subset: Literal['dev-clean', 'dev-other', 'test-clean', 'test-other']\n",
    "    granularity: Literal['triphone', 'phoneme']\n",
    "    speaker_mode: Literal['across', 'within']\n",
    "    context_mode: Literal['within', 'any']\n",
    "    score: float\n",
    "    pooling: str\n",
    "    seed: int\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return dict(\n",
    "            subset=self.subset,\n",
    "            granularity=self.granularity,\n",
    "            speaker_mode=self.speaker_mode,\n",
    "            context_mode=self.context_mode,\n",
    "            score=self.score,\n",
    "            pooling=self.pooling,\n",
    "            seed=self.seed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load leaderboard entries from the old abxLS without the scores section and indexed by their submission_id\"\"\"\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def fomat_scores(sc: Dict):\n",
    "    \"\"\" Format scores \"\"\"\n",
    "    return [\n",
    "        # dev-clean within/across speaker\n",
    "        ABXLSScore(\n",
    "            subset='dev-clean',\n",
    "            speaker_mode='within',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['clean']['dev']['within'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        ABXLSScore(\n",
    "            subset='dev-clean',\n",
    "            speaker_mode='across',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['clean']['dev']['across'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        # dev-other within/across speaker\n",
    "        ABXLSScore(\n",
    "            subset='dev-other',\n",
    "            speaker_mode='within',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['other']['dev']['within'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        ABXLSScore(\n",
    "            subset='dev-other',\n",
    "            speaker_mode='across',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['other']['dev']['across'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        # test-clean within across speaker\n",
    "        ABXLSScore(\n",
    "            subset='test-clean',\n",
    "            speaker_mode='within',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['clean']['test']['within'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        ABXLSScore(\n",
    "            subset='test-clean',\n",
    "            speaker_mode='across',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['clean']['test']['across'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        # test-other within/across speaker\n",
    "        ABXLSScore(\n",
    "            subset='test-other',\n",
    "            speaker_mode='within',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['other']['test']['within'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        ),\n",
    "        ABXLSScore(\n",
    "            subset='test-other',\n",
    "            speaker_mode='across',\n",
    "            context_mode='within',\n",
    "            granularity='triphone',\n",
    "            score=sc['other']['test']['across'],\n",
    "            pooling='none',\n",
    "            seed='none'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "old_leaderboard = {}\n",
    "for file in source_leaderboards.glob('*.yaml'):\n",
    "    with file.open() as fp:\n",
    "        data = yaml.load(fp, Loader=yaml.FullLoader)\n",
    "\n",
    "    submission_id = data.get('submission_id', None)\n",
    "    if submission_id is not None:\n",
    "        data['scores'] = fomat_scores(data['scores'])        \n",
    "        old_leaderboard[submission_id] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_scores(df: pd.DataFrame, subset: str):\n",
    "    # remove unused data\n",
    "    del df[\"path_data\"]\n",
    "    del df[\"item-file\"]\n",
    "    del df[\"dataset\"]\n",
    "    del df[\"sub-dataset\"]\n",
    "    # add granularity & subset\n",
    "    df['granularity'] = \"phoneme\"\n",
    "    df['subset'] = subset\n",
    "    # rename columns to be more clean\n",
    "    df = df.rename(\n",
    "        columns={'abx-s-condition': 'speaker_mode', 'abx-c-condition': 'context_mode'})\n",
    "    # reorder columns\n",
    "    cols = ['subset', 'speaker_mode', 'context_mode',\n",
    "            'granularity', 'score', 'pooling', 'seed']\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_submission(location: Path, sub_dir: str):\n",
    "    \"\"\" Load a submission from Marks evaluation format \"\"\"\n",
    "    location = location / sub_dir\n",
    "    scores_list = []\n",
    "\n",
    "    for _set in location.iterdir():\n",
    "        if _set.is_dir():\n",
    "            df = pd.read_csv(_set / 'ABX_scores.csv')\n",
    "            df = clean_scores(df, subset=_set.name)\n",
    "            scores_list.append(df)\n",
    "\n",
    "    df = pd.concat(scores_list)\n",
    "    df.reset_index(inplace=True)\n",
    "    del df['index']\n",
    "    return [ABXLSScore(**i) for i in df.to_dict(orient='records')]\n",
    "\n",
    "\n",
    "def load_all_submissions(location: Path, sub_dir: str):\n",
    "    \"\"\" Load all new scores from location \"\"\"\n",
    "    new_scores = {}\n",
    "    for item_location in location.iterdir():\n",
    "        if item_location.is_dir():\n",
    "            sub_id = item_location.name\n",
    "            new_scores[sub_id] = load_submission(item_location, sub_dir)\n",
    "    return new_scores\n",
    "\n",
    "\n",
    "subs_batch1 = Path(\n",
    "    '/scratch2/mhallap/zerospeech/evaluations/abx_evaluations/2022_evals/dtw_runs/v095/results/')\n",
    "subs_batch2 = Path(\n",
    "    '/scratch2/mhallap/zerospeech/evaluations/abx_evaluations/2022_evals/dtw_runs/v097')\n",
    "\n",
    "new_scores = load_all_submissions(subs_batch1, sub_dir='phonetic')\n",
    "new_scores.update(load_all_submissions(subs_batch2, sub_dir='input/phonetic'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 20211112161645_harwath !!!\n",
      "Not found 20211119235733_jaeyeonkim99 !!!\n",
      "Not found saurabh_bhati !!!\n",
      "Not found zr2021-baseline-random !!!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Build New leaderboard Objects \"\"\"\n",
    "\n",
    "\n",
    "def dump_scores(submission_id: str, score_list: List[ABXLSScore]):\n",
    "    \"\"\" Dump scores link to old leaderbord using submission_id to gather submission metadata \"\"\"\n",
    "    old = old_leaderboard.get(submission_id, None)\n",
    "\n",
    "    if old is None:\n",
    "        print(f'Not found {submission_id} !!!')\n",
    "        return\n",
    "    \n",
    "    # add new scores to old ones\n",
    "    score_list.extend(old['scores'])\n",
    "    # convert to dic & write as yaml\n",
    "    old['scores'] = [i if isinstance(i, dict) else i.to_dict() for i in score_list]\n",
    "    with (dest_dir / f\"{submission_id}.yaml\").open('w') as fp:\n",
    "        yaml.dump(old, fp)\n",
    "\n",
    "\n",
    "for sub_id, n_sc in new_scores.items():\n",
    "    dump_scores(sub_id, n_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Merge all entries \"\"\"\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "leaderboard_file = Path('/home/nhamilakis/workspace/zerospeech/core/leaderboards') / 'data/tasks/abx/abxLS-v2/abxLS-v2.json'\n",
    "\n",
    "data_list = []\n",
    "for f in dest_dir.glob('*.yaml'):\n",
    "    with f.open() as fp:\n",
    "        data_list.append(yaml.load(fp, Loader=yaml.FullLoader))\n",
    "\n",
    "\n",
    "with leaderboard_file.open('w') as fp:\n",
    "    json.dump(\n",
    "        dict(\n",
    "            updatedOn=datetime.now().isoformat(),\n",
    "            data=data_list\n",
    "        ), fp, indent=4\n",
    "    )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A quick script to fix score formatting \"\"\"\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def fix_score(dt):\n",
    "    def nested_dict():\n",
    "        return defaultdict(nested_dict)\n",
    "    \n",
    "    new_scores = nested_dict()\n",
    "    for entry in dt:\n",
    "        subset_type, subset_name = entry['subset'].split('-')\n",
    "        new_scores[entry['granularity']][entry['context_mode']][subset_name][entry['speaker_mode']][subset_type] = entry\n",
    "    \n",
    "    return new_scores\n",
    "\n",
    "# load leaderboard\n",
    "with leaderboard_file.open() as fp:\n",
    "    data = json.load(fp)\n",
    "    \n",
    "\n",
    "# fix scores\n",
    "entries = data.get('data')\n",
    "new_entries = []\n",
    "for entry in entries:\n",
    "    entry['scores'] = fix_score(entry['scores'])\n",
    "    new_entries.append(entry)\n",
    "    \n",
    "# save leaderboard\n",
    "with leaderboard_file.with_suffix(\".new.json\").open('w') as fp:\n",
    "    json.dump(dict(\n",
    "       updatedOn=datetime.now().isoformat(),\n",
    "       data=new_entries\n",
    "    ), fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zr-data",
   "language": "python",
   "name": "zr-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "14d21e145810db73aa00d31d36e7113cc77abaf4f6fa4e5308ccf64f188c0d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
