,used,model_id,bib_ref,authors,paper_title,paper_url,pub_date,code_url,task_list
0,TRUE,Bad15a,badino2015discovering,"Badino, L., Mereta, A. & Rosasco, L. ",Discovering discrete subword units with binarized autoencoders and hidden-Markov-model encoders,https://www.researchgate.net/profile/Leonardo-Badino/publication/281834458_Discovering_discrete_subword_units_with_Binarized_Autoencoders_and_Hidden-Markov-Model_Encoders/links/55fa662d08aeafc8ac38e6d4/Discovering-discrete-subword-units-with-Binarized-Autoencoders-and-Hidden-Markov-Model-Encoders.pdf,2015,-,ABX15
1,TRUE,Bad15b,badino2015discovering,"Badino, L., Mereta, A. & Rosasco, L. ",Discovering discrete subword units with binarized autoencoders and hidden-Markov-model encoders,https://www.researchgate.net/profile/Leonardo-Badino/publication/281834458_Discovering_discrete_subword_units_with_Binarized_Autoencoders_and_Hidden-Markov-Model_Encoders/links/55fa662d08aeafc8ac38e6d4/Discovering-discrete-subword-units-with-Binarized-Autoencoders-and-Hidden-Markov-Model-Encoders.pdf,2015,-,ABX15
2,TRUE,Bad15c,badino2015discovering,"Badino, L., Mereta, A. & Rosasco, L. ",Discovering discrete subword units with binarized autoencoders and hidden-Markov-model encoders,https://www.researchgate.net/profile/Leonardo-Badino/publication/281834458_Discovering_discrete_subword_units_with_Binarized_Autoencoders_and_Hidden-Markov-Model_Encoders/links/55fa662d08aeafc8ac38e6d4/Discovering-discrete-subword-units-with-Binarized-Autoencoders-and-Hidden-Markov-Model-Encoders.pdf,2015,-,ABX15
3,TRUE,Bal15a,baljekar2015using,"Baljekar, P., Sitaram, S., Muthukumar, P. & Black, A. ",Using articulatory features and inferred phonological segments in zero resource speech processing.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/baljekar15_interspeech.pdf,2015,-,ABX15
4,TRUE,Bal15b,baljekar2015using,"Baljekar, P., Sitaram, S., Muthukumar, P. & Black, A. ",Using articulatory features and inferred phonological segments in zero resource speech processing.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/baljekar15_interspeech.pdf,2015,-,ABX15
5,TRUE,Bal15c,baljekar2015using,"Baljekar, P., Sitaram, S., Muthukumar, P. & Black, A. ",Using articulatory features and inferred phonological segments in zero resource speech processing.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/baljekar15_interspeech.pdf,2015,-,ABX15
6,TRUE,Bal15d,baljekar2015using,"Baljekar, P., Sitaram, S., Muthukumar, P. & Black, A. ",Using articulatory features and inferred phonological segments in zero resource speech processing.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/baljekar15_interspeech.pdf,2015,-,ABX15
7,TRUE,Bal15e,baljekar2015using,"Baljekar, P., Sitaram, S., Muthukumar, P. & Black, A. ",Using articulatory features and inferred phonological segments in zero resource speech processing.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/baljekar15_interspeech.pdf,2015,-,ABX15
8,TRUE,Che15a,chen2015parallel,"Chen, H., Leung, C., Xie, L., Ma, B. & Li, H. ",Parallel inference of dirichlet process gaussian mixture models for unsupervised acoustic modeling: A feasibility study.,http://lxie.npu-aslp.org/papers/2015Interspeech_CHJ.pdf,2015,-,ABX15
9,TRUE,Che15b,chen2015parallel,"Chen, H., Leung, C., Xie, L., Ma, B. & Li, H. ",Parallel inference of dirichlet process gaussian mixture models for unsupervised acoustic modeling: A feasibility study.,http://lxie.npu-aslp.org/papers/2015Interspeech_CHJ.pdf,2015,-,ABX15
10,TRUE,Che15c,chen2015parallel,"Chen, H., Leung, C., Xie, L., Ma, B. & Li, H. ",Parallel inference of dirichlet process gaussian mixture models for unsupervised acoustic modeling: A feasibility study.,http://lxie.npu-aslp.org/papers/2015Interspeech_CHJ.pdf,2015,-,ABX15
11,TRUE,Che15d,chen2015parallel,"Chen, H., Leung, C., Xie, L., Ma, B. & Li, H. ",Parallel inference of dirichlet process gaussian mixture models for unsupervised acoustic modeling: A feasibility study.,http://lxie.npu-aslp.org/papers/2015Interspeech_CHJ.pdf,2015,-,ABX15
12,TRUE,Fah15,myrman2017partitioning,"Myrman, A. & Salvi, G. ",Partitioning of posteriorgrams using siamese models for unsupervised acoustic modelling.,https://www.isca-speech.org/archive_v0/GLU_2017/pdfs/GLU2017_paper_6.pdf,2017,-,ABX15
13,TRUE,Hec15a,heck2016unsupervised,"Heck, M., Sakti, S. & Nakamura, S.",Unsupervised linear discriminant analysis for supporting DPGMM clustering in the zero resource scenario.,https://reader.elsevier.com/reader/sd/pii/S1877050916300461?token=CB575EC5C1F3CB969515D6CCF4FAA80443F18E24F5304D5C3412D233B6EE1342FFAD1279FB2A17879E2158A97DCA9CF1&originRegion=eu-west-1&originCreation=20220608120801,2016,-,ABX15
14,TRUE,Hec15b,heck2016unsupervised,"Heck, M., Sakti, S. & Nakamura, S.",Unsupervised linear discriminant analysis for supporting DPGMM clustering in the zero resource scenario.,https://reader.elsevier.com/reader/sd/pii/S1877050916300461?token=CB575EC5C1F3CB969515D6CCF4FAA80443F18E24F5304D5C3412D233B6EE1342FFAD1279FB2A17879E2158A97DCA9CF1&originRegion=eu-west-1&originCreation=20220608120802,2016,-,ABX15
15,TRUE,Hec15c,heck2016unsupervised,"Heck, M., Sakti, S. & Nakamura, S.",Unsupervised linear discriminant analysis for supporting DPGMM clustering in the zero resource scenario.,https://reader.elsevier.com/reader/sd/pii/S1877050916300461?token=CB575EC5C1F3CB969515D6CCF4FAA80443F18E24F5304D5C3412D233B6EE1342FFAD1279FB2A17879E2158A97DCA9CF1&originRegion=eu-west-1&originCreation=20220608120803,2016,-,ABX15
16,TRUE,Hec15d,heck2016unsupervised,"Heck, M., Sakti, S. & Nakamura, S.",Unsupervised linear discriminant analysis for supporting DPGMM clustering in the zero resource scenario.,https://reader.elsevier.com/reader/sd/pii/S1877050916300461?token=CB575EC5C1F3CB969515D6CCF4FAA80443F18E24F5304D5C3412D233B6EE1342FFAD1279FB2A17879E2158A97DCA9CF1&originRegion=eu-west-1&originCreation=20220608120804,2016,-,ABX15
17,TRUE,Ren15a,renshaw2015comparison,"Renshaw, D., Kamper, H., Jansen, A. & Goldwater, S.",A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge.,https://homepages.inf.ed.ac.uk/sgwater/papers/interspeech15-NNMethods.pdf,2015,-,ABX15
18,TRUE,Ren15b,renshaw2015comparison,"Renshaw, D., Kamper, H., Jansen, A. & Goldwater, S.",A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge.,https://homepages.inf.ed.ac.uk/sgwater/papers/interspeech15-NNMethods.pdf,2015,-,ABX15
19,TRUE,Ren15c,renshaw2015comparison,"Renshaw, D., Kamper, H., Jansen, A. & Goldwater, S.",A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge.,https://homepages.inf.ed.ac.uk/sgwater/papers/interspeech15-NNMethods.pdf,2015,-,ABX15
20,TRUE,Ren15d,renshaw2015comparison,"Renshaw, D., Kamper, H., Jansen, A. & Goldwater, S.",A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge.,https://homepages.inf.ed.ac.uk/sgwater/papers/interspeech15-NNMethods.pdf,2015,-,ABX15
21,TRUE,Sri15a,srivastava2016articulatory,"Srivastava, B. & Shrivastava, M.",Articulatory gesture rich representation learning of phonological units in low resource settings. Springer.,https://www.researchgate.net/publication/308385411_Articulatory_Gesture_Rich_Representation_Learning_of_Phonological_Units_in_Low_Resource_Settings,2016,-,ABX15
22,TRUE,Sri15b,srivastava2016articulatory,"Srivastava, B. & Shrivastava, M.",Articulatory gesture rich representation learning of phonological units in low resource settings. Springer.,https://www.researchgate.net/publication/308385411_Articulatory_Gesture_Rich_Representation_Learning_of_Phonological_Units_in_Low_Resource_Settings,2016,-,ABX15
23,TRUE,Thi15,thiolliere2015hybrid,"Thiolliere, R., Dunbar, E., Synnaeve, G., Versteegh, M. & Dupoux, E.",A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling.,http://www.lscp.net/persons/dupoux/papers/Thioliere_DSVD_2015.Hybrid_DTW_DNN_acoustic_modeling.Interspeech.pdf,2015,-,ABX15
24,TRUE,Zeg15,zeghidour2016deep,"Zeghidour, N., Synnaeve, G., Versteegh, M. & Dupoux, E.",A deep scattering spectrum—deep siamese network pipeline for unsupervised acoustic modeling. ,http://www.lscp.net/persons/dupoux/papers/Zeghidour_SVD_2016_Deep_scat_ABnet.ICASSP.pdf,2016,-,ABX15
25,TRUE,Ans17a,ansari2017deep,"Ansari, T., Kumar, R., Singh, S. & Ganapathy, S.",Deep learning methods for unsupervised acoustic modeling—leap submission to zerospeech challenge 2017,https://ieeexplore.ieee.org/abstract/document/8269013,2017,-,ABX17
26,TRUE,Ans17b,ansari2017deep,"Ansari, T., Kumar, R., Singh, S. & Ganapathy, S.",Deep learning methods for unsupervised acoustic modeling—leap submission to zerospeech challenge 2018,https://ieeexplore.ieee.org/abstract/document/8269014,2017,-,ABX17
27,TRUE,Ans17c,ansari2017deep,"Ansari, T., Kumar, R., Singh, S. & Ganapathy, S.",Deep learning methods for unsupervised acoustic modeling—leap submission to zerospeech challenge 2019,https://ieeexplore.ieee.org/abstract/document/8269015,2017,-,ABX17
28,TRUE,Ans17d,ansari2017deep,"Ansari, T., Kumar, R., Singh, S. & Ganapathy, S.",Deep learning methods for unsupervised acoustic modeling—leap submission to zerospeech challenge 2020,https://ieeexplore.ieee.org/abstract/document/8269016,2017,-,ABX17
29,TRUE,Che17a,chen2017multilingual,"Chen, H., Leung, C., Xie, L., Ma, B. & Li, H. ",Multilingual bottle-neck feature learning from untranscribed speech.,https://ieeexplore.ieee.org/document/8269009,2017,-,ABX17
30,TRUE,Che17b,chen2017multilingual,"Chen, H., Leung, C., Xie, L., Ma, B. & Li, H. ",Multilingual bottle-neck feature learning from untranscribed speech.,https://ieeexplore.ieee.org/document/8269010,,-,ABX17
31,TRUE,Hec17a,heck2017feature,"Heck, M., Sakti, S. & Nakamura, S. ",Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017.,https://ieeexplore.ieee.org/document/8269011,2017,-,ABX17
32,TRUE,Hec17b,heck2017feature,"Heck, M., Sakti, S. & Nakamura, S. ",Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017.,https://ieeexplore.ieee.org/document/82690111,2017,-,ABX17
33,TRUE,Pel17a,pellegrini2017technical,"Pellegrini, T., Manenti, C. & Pinquier, J. ",Technical report the IRIT-UPS system@ ZeroSpeech 2017 Track1: Unsupervised subword modeling ,https://www.irit.fr/~Thomas.Pellegrini/pdf/tech-report-zrsc17-IRIT.pdf,2017,-,ABX17
34,TRUE,Pel17b,pellegrini2017technical,"Pellegrini, T., Manenti, C. & Pinquier, J. ",Technical report the IRIT-UPS system@ ZeroSpeech 2017 Track1: Unsupervised subword modeling ,https://www.irit.fr/~Thomas.Pellegrini/pdf/tech-report-zrsc17-IRIT.pdf,,-,ABX17
35,TRUE,Shi17a,shibata2017composite,"Shibata, H., Kato, T., Shinozaki, T. & Watanabet, S.",Composite embedding systems for ZeroSpeech2017 Track1. ,https://ieeexplore.ieee.org/abstract/document/8269012,2017,-,ABX17
36,TRUE,Shi17b,shibata2017composite,"Shibata, H., Kato, T., Shinozaki, T. & Watanabet, S.",Composite embedding systems for ZeroSpeech2017 Track1. ,https://ieeexplore.ieee.org/abstract/document/8269013,2017,-,ABX17
37,TRUE,Yua17a,yuan2017extracting,"Yuan, Y., Leung, C., Xie, L., Chen, H., Ma, B. & Li, H. ",Extracting bottleneck features and word-like pairs from untranscribed speech for feature representation. ,https://ieeexplore.ieee.org/document/8269010,2017,-,ABX17
38,TRUE,Yua17b,yuan2017extracting,"Yuan, Y., Leung, C., Xie, L., Chen, H., Ma, B. & Li, H. ",Extracting bottleneck features and word-like pairs from untranscribed speech for feature representation. ,https://ieeexplore.ieee.org/document/8269011,2017,-,ABX17
39,TRUE,Yua17c,yuan2017extracting,"Yuan, Y., Leung, C., Xie, L., Chen, H., Ma, B. & Li, H. ",Extracting bottleneck features and word-like pairs from untranscribed speech for feature representation. ,https://ieeexplore.ieee.org/document/8269012,2017,-,ABX17
40,TRUE,Cho19,chorowski2019unsupervised,"Chorowski, J., Weiss, R., Bengio, S. & Van Den Oord, A. ",Unsupervised speech representation learning using wavenet autoencoders.,https://arxiv.org/abs/1901.08810,,-,ABX17
41,FALSE,Kha20,kharitonov2021data,"Kharitonov, E., Rivière, M., Synnaeve, G., Wolf, L., Mazaré, P., Douze, M. & Dupoux, E. ",augmenting contrastive learning of speech representations in the time domain.,"https://arxiv.org/abs/2007.00991#:~:text=Data%20Augmenting%20Contrastive%20Learning%20of%20Speech%20Representations%20in%20the%20Time%20Domain,-Eugene%20Kharitonov,%20Morgane&text=Contrastive%20Predictive%20Coding%20(CPC),,representation%20learning%20of%20speech%20signal.",2020,https://github.com/facebookresearch/WavAugment,ABX17
42,TRUE,Baseline2,jansen2011efficient,"Jansen, A. & Van Durme, B.",Efficient spoken term discovery using randomized algorithms.,https://www.cs.jhu.edu/~vandurme/papers/JansenVanDurmeASRU11.pdf,2011,-,TDE15
43,TRUE,Kam15a,kamper2017embedded,"Kamper, H., Livescu, K. & Goldwater, S.",An embedded segmental k-means model for unsupervised segmentation and clustering of speech.,https://arxiv.org/abs/1703.08135,2017,-,TDE15
44,TRUE,Lyz15a,lyzinski2015evaluation,"Lyzinski, V., Sell, G. & Jansen, A.",An evaluation of graph clustering methods for unsupervised term discovery.,https://hltcoe.jhu.edu/wp-content/uploads/2016/10/Lyzinski_Sell_Jansen_2015A.pdf,2015,-,TDE15
45,TRUE,Lyz15b,lyzinski2015evaluation,"Lyzinski, V., Sell, G. & Jansen, A.",An evaluation of graph clustering methods for unsupervised term discovery.,https://hltcoe.jhu.edu/wp-content/uploads/2016/10/Lyzinski_Sell_Jansen_2015A.pdf,2015,-,TDE15
46,TRUE,Lyz15c,lyzinski2015evaluation,"Lyzinski, V., Sell, G. & Jansen, A.",An evaluation of graph clustering methods for unsupervised term discovery.,https://hltcoe.jhu.edu/wp-content/uploads/2016/10/Lyzinski_Sell_Jansen_2015A.pdf,2015,-,TDE15
47,TRUE,Ras15a,rasanen2015unsupervised,"Räsänen, O., Doyle, G. & Frank, M. ",Unsupervised word discovery from speech using automatic segmentation into syllable-like units.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/rasanen15_interspeech.pdf,2015,-,TDE15
48,TRUE,Ras15b,rasanen2015unsupervised,"Räsänen, O., Doyle, G. & Frank, M. ",Unsupervised word discovery from speech using automatic segmentation into syllable-like units.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/rasanen15_interspeech.pdf,2015,-,TDE15
49,TRUE,Ras15c,rasanen2015unsupervised,"Räsänen, O., Doyle, G. & Frank, M. ",Unsupervised word discovery from speech using automatic segmentation into syllable-like units.,https://www.isca-speech.org/archive/pdfs/interspeech_2015/rasanen15_interspeech.pdf,2015,-,TDE15
50,TRUE,Baseline2,jansen2011efficient,"Jansen, A. & Van Durme, B.",Efficient spoken term discovery using randomized algorithms.,https://www.cs.jhu.edu/~vandurme/papers/JansenVanDurmeASRU11.pdf,,-,TDE17
51,TRUE,Kam22,kamper2022word,"Kamper, H. ",Word segmentation on discovered phone units with dynamic programming and self-supervised scoring.,https://arxiv.org/abs/2202.11929,2022,-,TDE17
52,FALSE,Alg22,algayres22dpparse,"Algayres, R., Ricoul, T., Karadayi, J., Mohammed, A., Sagot, B. & Dupoux, E.",DP-PARSE: Finding word boundaries from raw speech with a token lexicon. Retrieved from https://arxiv.org/abs/1906.08237,https://arxiv.org/abs/1906.08237,2022,-,TDE17
53,TRUE,Kam17,kamper2017segmental,"Kamper, H., Jansen, A. & Goldwater, S. ", A segmental framework for fully-unsupervised large-vocabulary speech recognition.,https://arxiv.org/abs/1606.06950,2017,-,TDE17
54,TRUE,Bha20a,bhati2020self,"Bhati, S., Villalba, J., Żelasko, P. & Dehak, N.",Self-expressing autoencoders for unsupervised spoken term discovery.,https://arxiv.org/abs/2007.13033,2020,-,TDE17
55,TRUE,Bha20b,bhati2020self,"Bhati, S., Villalba, J., Żelasko, P. & Dehak, N.",Self-expressing autoencoders for unsupervised spoken term discovery.,https://arxiv.org/abs/2007.13033,2020,-,TDE17
56,TRUE,Gar17a,-,"Garcia F.,Sanchis E.",-,-,2017,-,TDE17
57,TRUE,Gar17b,-,"Garcia F.,Sanchis E.",-,-,2017,-,TDE17
58,TRUE,Ras20a,rasanen2020unsupervised,"Räsänen, O. & Blandón, M. ",Unsupervised discovery of recurring speech patterns using probabilistic adaptive metrics.,https://arxiv.org/abs/2008.00731,2020,-,TDE17
59,TRUE,Ras20b,rasanen2020unsupervised,"Räsänen, O. & Blandón, M. ",Unsupervised discovery of recurring speech patterns using probabilistic adaptive metrics.,https://arxiv.org/abs/2008.00732,2020,-,TDE17
60,TRUE,Ras17,seshadri2017comparison,"Seshadri, S., Remes, U., Räsänen, O. ",Comparison of non-parametric bayesian mixture models for syllable clustering and zero-resource speech processing.,https://www.researchgate.net/publication/317309206_Comparison_of_Non-Parametric_Bayesian_Mixture_Models_for_Syllable_Clustering_and_Zero-Resource_Speech_Processing,2017,-,"ABX17, TDE17"
61,FALSE,Iwa21,iwamoto2021unsupervised,"Iwamoto, Y. & Shinozaki, T.",Unsupervised spoken term discovery using wav2vec 2.0.,https://ieeexplore.ieee.org/document/9689644,2021,-,TDE17
62,TRUE,Baseline3,DBLP:conf/sltu/OndelBC16,"Ondel, L., Burget, L. & Cernocký, J.",Variational inference for acoustic unit discovery.,https://www.researchgate.net/publication/301827883_Variational_Inference_for_Acoustic_Unit_Discovery,2016,-,TTSO19
63,TRUE,Tja19a,tjandra2019speech,"Tjandra, A., Sakti, S. & Nakamura, S.",Speech-to-speech translation between untranscribed unknown languages.,https://arxiv.org/abs/1910.00795,2019,-,TTSO19
64,TRUE,Tja19b,tjandra2019speech,"Tjandra, A., Sakti, S. & Nakamura, S.",Speech-to-speech translation between untranscribed unknown languages.,https://arxiv.org/abs/1910.00796,2019,-,TTSO19
65,TRUE,Hor19a,-,-,-,-,2019,-,TTSO19
66,TRUE,Hor19b,-,-,-,-,2019,-,TTSO19
67,TRUE,Pan19a,murthy2020zero,"Pandia, K. & Murthy, H.",Zero resource speech synthesis using transcripts derived from perceptual acoustic units.,https://arxiv.org/abs/2006.04372,2020,-,TTSO19
68,TRUE,Pan19b,murthy2020zero,"Pandia, K. & Murthy, H.",Zero resource speech synthesis using transcripts derived from perceptual acoustic units.,https://arxiv.org/abs/2006.04373,2020,-,TTSO19
69,TRUE,Kam19a,eloff2019unsupervised,"Eloff, R., Nortje, A., Niekerk, B., Govender, A., Nortje, L., Pretorius, A., Van Biljon, E., Westhuizen, E., Staden, L. & Kamper, H.",Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.,https://arxiv.org/abs/1904.07556,2019,-,TTSO19
70,TRUE,Kam19b,eloff2019unsupervised,"Eloff, R., Nortje, A., Niekerk, B., Govender, A., Nortje, L., Pretorius, A., Van Biljon, E., Westhuizen, E., Staden, L. & Kamper, H.",Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.,https://arxiv.org/abs/1904.07557,2019,-,TTSO19
71,TRUE,Fen19a,feng,"Feng, S., Lee, T. & Peng, Z. ",Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.,https://arxiv.org/abs/1906.07234,2019,-,TTSO19
72,TRUE,Fen19b,feng,"Feng, S., Lee, T. & Peng, Z. ",Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.,https://arxiv.org/abs/1906.07235,2019,-,TTSO19
73,TRUE,Ral19,-,"Sai Krishna Rallabandi, Wenting Ye, Steven Hillis, Elizabeth Salesky, Alan W. Black",-,-,2019,-,TTSO19
74,TRUE,Cho19a,-,"Suhee Cho, Yeonjung Hong, Yookyung Shin, Youngsun Cho, Hyebin Yoon, Hyungwon Yang, Ingu Lee, Seohyun Kim, Wiback Kim, Youngjun Kim, Hosung Nam",-,-,2019,-,TTSO19
75,TRUE,Cho19b,-,"Suhee Cho, Yeonjung Hong, Yookyung Shin, Youngsun Cho, Hyebin Yoon, Hyungwon Yang, Ingu Lee, Seohyun Kim, Wiback Kim, Youngjun Kim, Hosung Nam",-,-,2019,-,TTSO19
76,TRUE,Liu19a,liu2019unsupervised,"Liu, A., Hsu, P. & Lee, H.",Unsupervised end-to-end learning of discrete linguistic units for voice conversion.,https://arxiv.org/abs/1905.11563,2019,-,TTSO19
77,TRUE,Liu19b,liu2019unsupervised,"Liu, A., Hsu, P. & Lee, H.",Unsupervised end-to-end learning of discrete linguistic units for voice conversion.,https://arxiv.org/abs/1905.11564,2019,-,TTSO19
78,TRUE,Kum19a,nayak2019virtual,"Nayak, S., Kumar, C., Ramesh, G., Bhati, S. & Murty, K.",Virtual phone discovery for speech synthesis without text.,https://ieeexplore.ieee.org/document/8969412,2019,-,TTSO19
79,TRUE,Kum19b,nayak2019virtual,"Nayak, S., Kumar, C., Ramesh, G., Bhati, S. & Murty, K.",Virtual phone discovery for speech synthesis without text.,https://ieeexplore.ieee.org/document/8969413,2019,-,TTSO19
80,TRUE,Yus19,yusuf2019temporally,"Yusuf, B., Gök, A., Gündogdu, B., Kose, O. & Saraclar, M. ",Temporally-aware acoustic unit discovery for zerospeech 2019 challenge.,https://www.researchgate.net/publication/335830054_Temporally-Aware_Acoustic_Unit_Discovery_for_Zerospeech_2019_Challenge,2019,-,TTSO19
81,TRUE,Gök19,yusuf2019hierarchical,"Yusuf, B., Gök, A., Gündogdu, B., Kose, O. & Saraclar, M. ",Temporally-aware acoustic unit discovery for zerospeech 2019 challenge.,https://www.researchgate.net/publication/335830054_Temporally-Aware_Acoustic_Unit_Discovery_for_Zerospeech_2019_Challenge,2019,-,TTSO19
82,TRUE,Gün20,gundogdu2020vector,"Gündogdu, B., Yusuf, B., Yesilbursa, M. & Saraclar, M.",Vector quantized temporally-aware correspondence sparse autoencoders for zero-resource acoustic unit discovery..,https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2765.pdf,2020,-,TTSO19
83,TRUE,Yus20a,yusuf2021hierarchical,"Yusuf, B., Ondel, L., Burget, L., Černockỳ, J. & Saraclar, M.",A hierarchical subspace model for language-attuned acoustic unit discovery.,https://arxiv.org/abs/2011.03115,2021,-,TTSO19
84,TRUE,Yus20b,yusuf2021hierarchical,"Yusuf, B., Ondel, L., Burget, L., Černockỳ, J. & Saraclar, M.",A hierarchical subspace model for language-attuned acoustic unit discovery.,https://arxiv.org/abs/2011.031155,2021,-,TTSO19
85,FALSE,Yus20c,yusuf2021hierarchical,"Yusuf, B., Ondel, L., Burget, L., Černockỳ, J. & Saraclar, M.",A hierarchical subspace model for language-attuned acoustic unit discovery.,https://arxiv.org/abs/2011.03115,2021,-,TTSO19
86,TRUE,Hou20a,-,"Wenxin Hou, Mingxin Zhang, Shengzhou Gao, Takahiro Shinozaki",-,-,2020,-,TTSO19
87,TRUE,Hou20b,-,"Wenxin Hou, Mingxin Zhang, Shengzhou Gao, Takahiro Shinozaki",-,-,2020,-,TTSO19
88,TRUE,Mor20,-,"Takashi Morita, Hiroki Koda",-,-,,-,TTSO19
89,TRUE,Kum20a,prakash2020exploration,"Prakash, A., Kumar, M., Murthy, H.",Exploration of end-to-end synthesisers for zero resource speech challenge 2020.,https://arxiv.org/abs/2009.04983,2020,-,TTSO19
90,TRUE,Kum20b,prakash2020exploration,"Prakash, A., Kumar, M., Murthy, H.",Exploration of end-to-end synthesisers for zero resource speech challenge 2020.,https://arxiv.org/abs/2009.04984,2020,-,TTSO19
91,TRUE,Che20a,chen2020unsupervised,"Chen, M. & Hain, T. ",Unsupervised acoustic unit representation learning for voice conversion using wavenet auto-encoders.,https://arxiv.org/abs/2008.06892,2020,-,TTSO19
92,TRUE,Che20b,chen2020unsupervised,"Chen, M. & Hain, T. ",Unsupervised acoustic unit representation learning for voice conversion using wavenet auto-encoders.,https://arxiv.org/abs/2008.06893,2020,-,TTSO19
93,TRUE,Lum20a,tobing2020cyclic,"Tobing, P., Hayashi, T., Wu, Y., Kobayashi, K. & Toda, T. ",Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling.,https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2559.pdf,2020,-,TTSO19
94,TRUE,Lum20b,tobing2020cyclic,"Tobing, P., Hayashi, T., Wu, Y., Kobayashi, K. & Toda, T. ",Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling.,https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2559.pdf,2020,-,TTSO19
95,TRUE,Nie20a,van2020vector,"Niekerk, B., Nortje, L. & Kamper, H.",Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge.,https://arxiv.org/abs/2005.09409,2020,-,TTSO19
96,TRUE,Nie20b,van2020vector,"Niekerk, B., Nortje, L. & Kamper, H.",Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge.,https://arxiv.org/abs/2005.09410,2020,-,TTSO19
97,TRUE,Tja20a,tjandra2020transformer,"Tjandra, A., Sakti, S. & Nakamura, S.",Transformer vq-vae for unsupervised unit discovery and speech synthesis: Zerospeech 2020 challenge.,https://arxiv.org/abs/2005.11676,2020,-,TTSO19
98,TRUE,Tja20b,tjandra2020transformer,"Tjandra, A., Sakti, S. & Nakamura, S.",Transformer vq-vae for unsupervised unit discovery and speech synthesis: Zerospeech 2020 challenge.,https://arxiv.org/abs/2005.11677,2020,-,TTSO19
99,TRUE,Baseline4-sm,nguyen2020zero,"Nguyen, T., Seyssel, M., Rozé, P., Rivière, M., Kharitonov, E., Baevski, A., Dunbar, E. & Dupoux, E. ",The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling.,https://arxiv.org/abs/2011.11588,2020,-,"sLM21, ABXLS"
100,TRUE,Baseline4-lg,nguyen2020zero,"Nguyen, T., Seyssel, M., Rozé, P., Rivière, M., Kharitonov, E., Baevski, A., Dunbar, E. & Dupoux, E. ",The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling.,https://arxiv.org/abs/2011.11589,2020,-,"sLM21, ABXLS"
101,TRUE,Baseline4vg-sm,alishahi2021zr,"Alishahi, A., Chrupała, G., Cristia, A., Dupoux, E., Higy, B., Lavechin, M., Räsänen, O. & Yu, C.","R-2021VG: Zero-resource speech challenge, visually-grounded language modelling track.",https://arxiv.org/abs/2107.06546,2021,-,"sLM21, ABXLS"
102,TRUE,Baseline4vg-lg,alishahi2021zr,"Alishahi, A., Chrupała, G., Cristia, A., Dupoux, E., Higy, B., Lavechin, M., Räsänen, O. & Yu, C.","R-2021VG: Zero-resource speech challenge, visually-grounded language modelling track.",https://arxiv.org/abs/2107.06547,2021,-,"sLM21, ABXLS"
103,TRUE,Nie21,van2021analyzing,"Niekerk, B., Nortje, L., Baas, M. & Kamper, H. ",Analyzing speaker information in self-supervised models to improve zero-resource speech processing.,https://arxiv.org/abs/2108.00917,2021,-,"sLM21, ABXLS"
104,TRUE,Cho21a,chorowski2021information,"Chorowski, J., Ciesielski, G., Dzikowski, J., Łańcucki, A., Marxer, R., Opala, M., Pusz, P., Rychlikowski, P. & Stypułkowski, M.",Information retrieval for zerospeech 2021: The submission by university of wroclaw.,https://arxiv.org/abs/2106.11603,2021,-,"sLM21, ABXLS"
105,TRUE,Cho21b,chorowski2021information,"Chorowski, J., Ciesielski, G., Dzikowski, J., Łańcucki, A., Marxer, R., Opala, M., Pusz, P., Rychlikowski, P. & Stypułkowski, M.",Information retrieval for zerospeech 2021: The submission by university of wroclaw.,https://arxiv.org/abs/2106.11604,2021,-,"sLM21, ABXLS"
106,TRUE,Liu21,-,liu,-,-,2021,-,"sLM21, ABXLS"
107,TRUE,Mae21a,maekaku2021speech,"Maekaku, T., Chang, X., Fujita, Y., Chen, L., Watanabe, S. & Rudnicky, A. ",Speech representation learning combining conformer cpc with deep cluster for the zerospeech challenge 2021.,https://arxiv.org/abs/2107.05899,2021,-,"sLM21, ABXLS"
108,TRUE,Mae21b,maekaku2021speech,"Maekaku, T., Chang, X., Fujita, Y., Chen, L., Watanabe, S. & Rudnicky, A. ",Speech representation learning combining conformer cpc with deep cluster for the zerospeech challenge 2021.,https://arxiv.org/abs/2107.05900,2021,-,"sLM21, ABXLS"
109,TRUE,Mae21c,maekaku2021speech,"Maekaku, T., Chang, X., Fujita, Y., Chen, L., Watanabe, S. & Rudnicky, A. ",Speech representation learning combining conformer cpc with deep cluster for the zerospeech challenge 2021.,https://arxiv.org/abs/2107.05901,2021,-,"sLM21, ABXLS"
110,TRUE,Mae21d,maekaku2021speech,"Maekaku, T., Chang, X., Fujita, Y., Chen, L., Watanabe, S. & Rudnicky, A. ",Speech representation learning combining conformer cpc with deep cluster for the zerospeech challenge 2021.,https://arxiv.org/abs/2107.05902,2021,-,"sLM21, ABXLS"
111,TRUE,Pen21,peng2022self,"Peng, P. & Harwath, D. ",Self-supervised representation learning for speech using visual grounding and masked language modeling.,https://arxiv.org/abs/2202.03543,2021,-,"sLM21, ABXLS"
112,TRUE,Lee21a,-,"Kyogu Lee, Jaeyeon Kim, Injune Hwang",-,-,2021,-,"sLM21, ABXLS"
113,TRUE,Lee21b,-,"Kyogu Lee, Jaeyeon Kim, Injune Hwang",-,-,2021,-,"sLM21, ABXLS"
114,TRUE,Gan21a,-,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
115,TRUE,Gan21b,-,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
116,TRUE,Gan21c,-,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
117,TRUE,Gan21d,-,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
118,TRUE,Gan21e,,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
119,TRUE,Gan21f,,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
120,TRUE,Gan21g,,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
121,TRUE,Gan21h,,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
122,TRUE,Gan21i,,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
123,TRUE,Gan21j,,"Dr.Sriram Ganapathy, Varun Krishna",-,-,2021,-,"sLM21, ABXLS"
124,TRUE,Ngu21a,nguyen22discrete,"Nguyen, T., Sagot, B. & Dupoux, E. ",Are discrete units necessary for spoken language modeling? ,https://arxiv.org/abs/2203.05936,2021,-,"sLM21, ABXLS"
125,TRUE,Ngu21b,nguyen22discrete,"Nguyen, T., Sagot, B. & Dupoux, E. ",Are discrete units necessary for spoken language modeling? ,https://arxiv.org/abs/2203.05937,2021,-,"sLM21, ABXLS"
126,TRUE,Ngu21c,nguyen22discrete,"Nguyen, T., Sagot, B. & Dupoux, E. ",Are discrete units necessary for spoken language modeling? ,https://arxiv.org/abs/2203.05938,2021,-,"sLM21, ABXLS"
127,TRUE,Ngu21d,nguyen22discrete,"Nguyen, T., Sagot, B. & Dupoux, E. ",Are discrete units necessary for spoken language modeling? ,https://arxiv.org/abs/2203.05939,2021,-,"sLM21, ABXLS"
128,TRUE,Łań21,-,"Adrian Łancucki, Tomasz Grzegorzek, Santiago Cuervo, Ricard Marxer, Paweł Rychlikowski, Jan Chorowski",-,-,2021,-,"sLM21, ABXLS"
129,TRUE,Bha21a,bhati2021unsupervised,"Bhati, S., Villalba, J., Żelasko, P., Moro-Velazquez, L. & Dehak, N.",Unsupervised speech segmentation and variable rate representation learning using segmental contrastive predictive coding.,https://arxiv.org/abs/2110.02345,2021,-,"sLM21, ABXLS"
130,TRUE,Bha21b,bhati2021unsupervised,"Bhati, S., Villalba, J., Żelasko, P., Moro-Velazquez, L. & Dehak, N.",Unsupervised speech segmentation and variable rate representation learning using segmental contrastive predictive coding.,https://arxiv.org/abs/2110.02345,2021,-,"sLM21, ABXLS"
131,TRUE,Gao21a,-,"Lingyun Gao, Siyuan Feng, Odette Scharenborg",-,-,2021,-,"sLM21, ABXLS"
132,TRUE,Gao21b,-,"Lingyun Gao, Siyuan Feng, Odette Scharenborg",-,-,2021,-,"sLM21, ABXLS"
133,TRUE,Gao21c,-,"Lingyun Gao, Siyuan Feng, Odette Scharenborg",-,-,2021,-,"sLM21, ABXLS"
