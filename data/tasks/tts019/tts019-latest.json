{
    "last_modified": "2022-07-27T17:06:31.479652",
    "data": [
        {
            "model_id": "Liu19a",
            "submission_id": "20190316094151_andi611",
            "index": 18,
            "submission_date": "2019-03-16T09:41:51+00:00",
            "submitted_by": "andi611",
            "description": "We present an end-to-end training scheme where we discover discrete subword units from speech in an unsupervised way, then we use these discovered linguistic units to train a text-to-speech model, without any text transcript. We propose an discrete encoding called Multilabel-Binary Vectors (MBV) for subword units discovery, which delivers a strong bottleneck for disentangling speech content and speaker style, and is sufficient to represent all the phonemes in a given language. MBV can be learned under an ASR-TTS auto-encoder reconstruction setting, where an ASR-Encoder is trained to discover a set of common linguistic units given a variety of speakers, and a TTS- Decoder trained to project the discovered units back to the original speech. The proposed method was able to encode a whole language down to phoneme-level with just 64 distinct units. As a result, we were able to perform voice conversion between speakers with theses units, using the ASR-Encoder and TTS-Decoder alone. Furthermore, we improve the quality of voice conversion using a second stage adversarial training, where we train a TTS-patcher that augments the output of TTS-Decoder. Voice conversion results shows that the discovered hidden units are successful in encoding speech content and well disentangled from style. We will describe our method in detail by submitting a paper to the Interspeech 2019 Special Workshop, using the author names provided above.",
            "publication": {
                "author_short": "Liu <i>et al.</i>",
                "authors": "Liu, A., Hsu, P. & Lee, H.",
                "paper_title": "Unsupervised end-to-end learning of discrete linguistic units for voice conversion.",
                "paper_ref": "Liu, A., Hsu, P. & Lee, H. (2019.0) Unsupervised end-to-end learning of discrete linguistic units for voice conversion.",
                "bib_ref": "liu2019unsupervised",
                "paper_url": "https://arxiv.org/abs/1905.11563",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "College of Electrical Engineering and Computer Science, National Taiwan University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_kl",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.3,
                    "cer": 0.92,
                    "similarity": 1.62,
                    "abx": 39.23,
                    "bitrate": 48.78
                },
                "austronesian": {
                    "mos": 1.27,
                    "cer": 0.86,
                    "similarity": 1.96,
                    "abx": 43.42,
                    "bitrate": 43.95
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190316094151_andi611.wav",
                        "sample_2": "english_2_20190316094151_andi611.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190316094151_andi611.wav",
                        "sample_2": "surprise_2_20190316094151_andi611.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 39.03,
                            "test_abx_dtw_kl": 39.23,
                            "test_abx_levenshtein": 41.99
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 43.2,
                            "test_abx_dtw_kl": 43.42,
                            "test_abx_levenshtein": 44.25
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 48.78
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 43.95
                        }
                    }
                },
                "extra_description": [
                    "None is used",
                    "None is used"
                ]
            }
        },
        {
            "model_id": "Hor19a",
            "submission_id": "20190314153658_yibin",
            "index": 4,
            "submission_date": "2019-03-14T15:36:58+00:00",
            "submitted_by": "yibin",
            "description": "none",
            "publication": {
                "author_short": "Horizon Robotics",
                "authors": "-",
                "paper_title": "-",
                "paper_ref": "- (2019.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Horizion Robotics",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.19,
                    "cer": 0.92,
                    "similarity": 1.22,
                    "abx": 30.5,
                    "bitrate": 832.58
                },
                "austronesian": {
                    "mos": 2.89,
                    "cer": 0.36,
                    "similarity": 1.43,
                    "abx": 24.92,
                    "bitrate": 842.46
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190314153658_yibin.wav",
                        "sample_2": "english_2_20190314153658_yibin.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190314153658_yibin.wav",
                        "sample_2": "surprise_2_20190314153658_yibin.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 30.5,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 44.32
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 24.92,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 47.17
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 832.58
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 842.46
                        }
                    }
                },
                "extra_description": [
                    "Embeddings discovered by the Ondel algorithm, before conversion to<br>one-hot vectors",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Fen19a",
            "submission_id": "20190314161322_syfeng",
            "index": 5,
            "submission_date": "2019-03-14T16:13:22+00:00",
            "submitted_by": "syfeng",
            "description": "The only difference between this system and our team's other system is that, we perform text filtering towards test ont-hot embeddings. The motivation is to filter out 'very short' symbolic representation. This should reduce bit rate of test embeddings. As this process is done towards test embedding only, auxiliary embeddings do not change as compared to our another system.",
            "publication": {
                "author_short": "Feng <i>et al.</i>",
                "authors": "Feng, S., Lee, T. & Peng, Z. ",
                "paper_title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.",
                "paper_ref": "Feng, S., Lee, T. & Peng, Z.  (2019.0) Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.",
                "bib_ref": "feng",
                "paper_url": "https://arxiv.org/abs/1906.07234",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "The Chinese University of Hong Kong",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_kl",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.78,
                    "cer": 0.76,
                    "similarity": 2.33,
                    "abx": 25.3,
                    "bitrate": 320.01
                },
                "austronesian": {
                    "mos": 1.67,
                    "cer": 0.66,
                    "similarity": 2.6,
                    "abx": 16.87,
                    "bitrate": 299.21
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190314161322_syfeng.wav",
                        "sample_2": "english_2_20190314161322_syfeng.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190314161322_syfeng.wav",
                        "sample_2": "surprise_2_20190314161322_syfeng.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 25.87,
                            "test_abx_dtw_kl": 25.3,
                            "test_abx_levenshtein": 28.59
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 17.65,
                            "test_abx_dtw_kl": 16.87,
                            "test_abx_levenshtein": 23.98
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 320.01
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 299.21
                        }
                    }
                },
                "extra_description": [
                    "-",
                    "-"
                ]
            }
        },
        {
            "model_id": "Pan19a",
            "submission_id": "20190314064547_pandia",
            "index": 3,
            "submission_date": "2019-03-14T06:45:47+00:00",
            "submitted_by": "pandia",
            "description": "Speech units of the form CV, VC transients and steady-state regions are modelled using GMM-HMM",
            "publication": {
                "author_short": "Pandia <i>et al.</i>",
                "authors": "Pandia, K. & Murthy, H.",
                "paper_title": "Zero resource speech synthesis using transcripts derived from perceptual acoustic units.",
                "paper_ref": "Pandia, K. & Murthy, H. (2020.0) Zero resource speech synthesis using transcripts derived from perceptual acoustic units.",
                "bib_ref": "murthy2020zero",
                "paper_url": "https://arxiv.org/abs/2006.04372",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Indian Institute of Technology Madras",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.77,
                    "cer": 0.61,
                    "similarity": 3.0,
                    "abx": 28.16,
                    "bitrate": 92.75
                },
                "austronesian": {
                    "mos": 2.02,
                    "cer": 0.48,
                    "similarity": 3.21,
                    "abx": 20.77,
                    "bitrate": 94.15
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190314064547_pandia.wav",
                        "sample_2": "english_2_20190314064547_pandia.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190314064547_pandia.wav",
                        "sample_2": "surprise_2_20190314064547_pandia.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 28.16,
                            "test_abx_dtw_kl": 27.42,
                            "test_abx_levenshtein": 29.07
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 20.77,
                            "test_abx_dtw_kl": 19.76,
                            "test_abx_levenshtein": 23.63
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 92.75
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 94.15
                        }
                    }
                },
                "extra_description": [
                    "-",
                    "-"
                ]
            }
        },
        {
            "model_id": "Liu19b",
            "submission_id": "20190315081109_andi611",
            "index": 6,
            "submission_date": "2019-03-15T08:11:09+00:00",
            "submitted_by": "andi611",
            "description": "We present an end-to-end training scheme where we discover discrete linguistic subword units from speech in an unsupervised way, and we use these discovered units to train a text-to-speech model, without any text transcript. We propose an discrete encoding called multilabel-binary vectors (MBV) for subword units discovery, which delivers a strong bottleneck for disentangling speech content and speaker style, and is sufficient to represent all the phonemes in a given language. MBV can be learned under an ASR-TTS auto-encoder reconstruction setting, where an ASR-Encoder is trained to discover a set of common linguistic units given a variety of speakers, and a TTS- Decoder trained to project the discovered units back to the original speech. The proposed method was able to encode a whole language down to phoneme-level with just 64 distinct units. As a result, we were able to perform voice conversion between speakers with theses units, using the ASR-Encoder and TTS-Decoder alone. We will describe our method in detail by submitting a paper to the Interspeech 2019 Special Workshop, using the author names provided above.",
            "publication": {
                "author_short": "Liu <i>et al.</i>",
                "authors": "Liu, A., Hsu, P. & Lee, H.",
                "paper_title": "Unsupervised end-to-end learning of discrete linguistic units for voice conversion.",
                "paper_ref": "Liu, A., Hsu, P. & Lee, H. (2019.0) Unsupervised end-to-end learning of discrete linguistic units for voice conversion.",
                "bib_ref": "liu2019unsupervised",
                "paper_url": "https://arxiv.org/abs/1905.11564",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "College of Electrical Engineering and Computer Science, National Taiwan University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.22,
                    "cer": 0.89,
                    "similarity": 1.77,
                    "abx": 41.99,
                    "bitrate": 48.78
                },
                "austronesian": {
                    "mos": 1.69,
                    "cer": 0.81,
                    "similarity": 1.97,
                    "abx": 44.25,
                    "bitrate": 43.95
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315081109_andi611.wav",
                        "sample_2": "english_2_20190315081109_andi611.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315081109_andi611.wav",
                        "sample_2": "surprise_2_20190315081109_andi611.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 39.03,
                            "test_abx_dtw_kl": 39.23,
                            "test_abx_levenshtein": 41.99
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 43.2,
                            "test_abx_dtw_kl": 43.42,
                            "test_abx_levenshtein": 44.25
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 48.78
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 43.95
                        }
                    }
                },
                "extra_description": [
                    "None is used",
                    "None is used"
                ]
            }
        },
        {
            "model_id": "Kum19a",
            "submission_id": "20190315122333_shekharnayak",
            "index": 10,
            "submission_date": "2019-03-15T12:23:33+00:00",
            "submitted_by": "shekharnayak",
            "description": "Features - MFCC, Kernel-gram segmentation, k-means clustering, Merlin speech synthesizer",
            "publication": {
                "author_short": "Kumar <i>et al.</i>",
                "authors": "Nayak, S., Kumar, C., Ramesh, G., Bhati, S. & Murty, K.",
                "paper_title": "Virtual phone discovery for speech synthesis without text.",
                "paper_ref": "Nayak, S., Kumar, C., Ramesh, G., Bhati, S. & Murty, K. (2019.0) Virtual phone discovery for speech synthesis without text.",
                "bib_ref": "nayak2019virtual",
                "paper_url": "https://ieeexplore.ieee.org/document/8969412",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Indian Institute of Technology Hyderabad, Johns Hopkins University",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.18,
                    "cer": 0.83,
                    "similarity": 3.01,
                    "abx": 45.54,
                    "bitrate": 52.21
                },
                "austronesian": {
                    "mos": 1.82,
                    "cer": 0.86,
                    "similarity": 3.3,
                    "abx": 40.17,
                    "bitrate": 46.07
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315122333_shekharnayak.wav",
                        "sample_2": "english_2_20190315122333_shekharnayak.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315122333_shekharnayak.wav",
                        "sample_2": "surprise_2_20190315122333_shekharnayak.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 46.33,
                            "auxiliary1_abx_dtw_kl": 45.98,
                            "auxiliary1_abx_levenshtein": 45.54,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 46.33,
                            "test_abx_dtw_kl": 45.98,
                            "test_abx_levenshtein": 45.54
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 39.07,
                            "auxiliary1_abx_dtw_kl": 37.97,
                            "auxiliary1_abx_levenshtein": 40.17,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 39.07,
                            "test_abx_dtw_kl": 37.97,
                            "test_abx_levenshtein": 40.17
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 52.21,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 52.21
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 46.07,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 46.07
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Kum19b",
            "submission_id": "20190315121929_shekharnayak",
            "index": 9,
            "submission_date": "2019-03-15T12:19:29+00:00",
            "submitted_by": "shekharnayak",
            "description": "Features - Bottleneck, Kernel-gram segmentation, k-means clustering, Merlin speech synthesizer",
            "publication": {
                "author_short": "Kumar <i>et al.</i>",
                "authors": "Nayak, S., Kumar, C., Ramesh, G., Bhati, S. & Murty, K.",
                "paper_title": "Virtual phone discovery for speech synthesis without text.",
                "paper_ref": "Nayak, S., Kumar, C., Ramesh, G., Bhati, S. & Murty, K. (2019.0) Virtual phone discovery for speech synthesis without text.",
                "bib_ref": "nayak2019virtual",
                "paper_url": "https://ieeexplore.ieee.org/document/8969413",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Indian Institute of Technology Hyderabad, Johns Hopkins University",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.84,
                    "cer": 0.92,
                    "similarity": 2.59,
                    "abx": 47.41,
                    "bitrate": 37.22
                },
                "austronesian": {
                    "mos": 1.44,
                    "cer": 0.89,
                    "similarity": 3.02,
                    "abx": 45.64,
                    "bitrate": 44.07
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315121929_shekharnayak.wav",
                        "sample_2": "english_2_20190315121929_shekharnayak.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315121929_shekharnayak.wav",
                        "sample_2": "surprise_2_20190315121929_shekharnayak.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 48.03,
                            "auxiliary1_abx_dtw_kl": 48.02,
                            "auxiliary1_abx_levenshtein": 47.41,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 48.03,
                            "test_abx_dtw_kl": 48.02,
                            "test_abx_levenshtein": 47.41
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 46.16,
                            "auxiliary1_abx_dtw_kl": 47.29,
                            "auxiliary1_abx_levenshtein": 45.64,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 46.16,
                            "test_abx_dtw_kl": 47.29,
                            "test_abx_levenshtein": 45.64
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 37.22,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 37.22
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 44.07,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 44.07
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Tja19a",
            "submission_id": "20190316181330_Sakti",
            "index": 20,
            "submission_date": "2019-03-16T18:13:30+00:00",
            "submitted_by": "Sakti",
            "description": "We proposed VQVAE + Multiscale inverter + GAN enhancement approach. Input -- MFCC features (including deltas) with 39 dimension. VQVAE (To generate embedding) -- 256 codebook, 64 embedding dimension, 2x time reduction. Inverter (To generate speech from codebook) -- multiscale 1D convolution. GAN (To enhance inverter output)",
            "publication": {
                "author_short": "Tjandra <i>et al.</i>",
                "authors": "Tjandra, A., Sakti, S. & Nakamura, S.",
                "paper_title": "Speech-to-speech translation between untranscribed unknown languages.",
                "paper_ref": "Tjandra, A., Sakti, S. & Nakamura, S. (2019.0) Speech-to-speech translation between untranscribed unknown languages.",
                "bib_ref": "tjandra2019speech",
                "paper_url": "https://arxiv.org/abs/1910.00795",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "NAIST (Japan), RIKEN AIP (Japan), NUS (Singapore)",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.2,
                    "cer": 0.42,
                    "similarity": 1.98,
                    "abx": 24.38,
                    "bitrate": 349.41
                },
                "austronesian": {
                    "mos": 3.2,
                    "cer": 0.21,
                    "similarity": 2.3,
                    "abx": 13.98,
                    "bitrate": 362.99
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190316181330_Sakti.wav",
                        "sample_2": "english_2_20190316181330_Sakti.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190316181330_Sakti.wav",
                        "sample_2": "surprise_2_20190316181330_Sakti.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 24.38,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 39.11
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 13.98,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 38.73
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 349.41
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 362.99
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Baseline3",
            "submission_id": "20190205160754_mmmaat",
            "index": 1,
            "submission_date": "2019-02-05T16:07:54+00:00",
            "submitted_by": "mmmaat",
            "description": "<a href=\"https://zerospeech.com/2019/getting_started.html#baseline-system\">Baseline of ZeroSpeech 2019</a>",
            "publication": {
                "author_short": "<b>Baseline</b>",
                "authors": "Ondel, L., Burget, L. & Cernock\u00fd, J.",
                "paper_title": "Variational inference for acoustic unit discovery.",
                "paper_ref": "Ondel, L., Burget, L. & Cernock\u00fd, J. (2016.0) Variational inference for acoustic unit discovery.",
                "bib_ref": "DBLP:conf/sltu/OndelBC16",
                "paper_url": "https://www.researchgate.net/publication/301827883_Variational_Inference_for_Acoustic_Unit_Discovery",
                "pub_year": 2016.0,
                "team_name": null,
                "institution": "PSL University, CNRS, ENS, EHESS, INRIA",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.5,
                    "cer": 0.75,
                    "similarity": 2.97,
                    "abx": 35.63,
                    "bitrate": 71.98
                },
                "austronesian": {
                    "mos": 2.07,
                    "cer": 0.62,
                    "similarity": 3.41,
                    "abx": 27.46,
                    "bitrate": 74.55
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190205160754_mmmaat.wav",
                        "sample_2": "english_2_20190205160754_mmmaat.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190205160754_mmmaat.wav",
                        "sample_2": "surprise_2_20190205160754_mmmaat.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 34.66,
                            "auxiliary1_abx_dtw_kl": 34.51,
                            "auxiliary1_abx_levenshtein": 33.28,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 35.63,
                            "test_abx_dtw_kl": 35.7,
                            "test_abx_levenshtein": 34.74
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 27.75,
                            "auxiliary1_abx_dtw_kl": 27.65,
                            "auxiliary1_abx_levenshtein": 29.14,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 27.46,
                            "test_abx_dtw_kl": 27.72,
                            "test_abx_levenshtein": 29.21
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 561.47,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 71.98
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 556.41,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 74.55
                        }
                    }
                },
                "extra_description": [
                    "Embeddings discovered by the Ondel algorithm, before conversion to<br>one-hot vectors",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Kam19a",
            "submission_id": "20190315135759_kamperh",
            "index": 12,
            "submission_date": "2019-03-15T13:57:59+00:00",
            "submitted_by": "kamperh",
            "description": "A neural network model with intermediate discritisation is trained to produce speech features (filterbanks) as output given MFCC features as input. The output is used to condition a trained neural vocoder (FFTNet). Internal label, ryan_conv_deconv_compression_8.",
            "publication": {
                "author_short": "Kamper <i>et al.</i>",
                "authors": "Eloff, R., Nortje, A., Niekerk, B., Govender, A., Nortje, L., Pretorius, A., Van Biljon, E., Westhuizen, E., Staden, L. & Kamper, H.",
                "paper_title": "Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.",
                "paper_ref": "Eloff, R., Nortje, A., Niekerk, B., Govender, A., Nortje, L., Pretorius, A., Van Biljon, E., Westhuizen, E., Staden, L. & Kamper, H. (2019.0) Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.",
                "bib_ref": "eloff2019unsupervised",
                "paper_url": "https://arxiv.org/abs/1904.07556",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Stellenbosch University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.31,
                    "cer": 0.75,
                    "similarity": 2.49,
                    "abx": 30.78,
                    "bitrate": 87.74
                },
                "austronesian": {
                    "mos": 1.94,
                    "cer": 0.58,
                    "similarity": 1.95,
                    "abx": 26.49,
                    "bitrate": 69.22
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315135759_kamperh.wav",
                        "sample_2": "english_2_20190315135759_kamperh.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315135759_kamperh.wav",
                        "sample_2": "surprise_2_20190315135759_kamperh.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 27.14,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 45.5,
                            "auxiliary2_abx_dtw_cosine": 25.1,
                            "auxiliary2_abx_dtw_kl": 26.62,
                            "auxiliary2_abx_levenshtein": 44.26,
                            "test_abx_dtw_cosine": 30.78,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 41.44
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 23.5,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 47.02,
                            "auxiliary2_abx_dtw_cosine": 17.6,
                            "auxiliary2_abx_dtw_kl": 28.15,
                            "auxiliary2_abx_levenshtein": 47.05,
                            "test_abx_dtw_cosine": 26.49,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 39.26
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 188.4,
                            "auxiliary2_bitrate": 1732.09,
                            "test_bitrate": 87.74
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 186.05,
                            "auxiliary2_bitrate": 1745.19,
                            "test_bitrate": 69.22
                        }
                    }
                },
                "extra_description": [
                    "Intermediate continuous features prior to discretisation.",
                    "Intermediate continuous features prior to vocoding but after<br>discretisation."
                ]
            }
        },
        {
            "model_id": "Hor19b",
            "submission_id": "20190315114628_yibin",
            "index": 8,
            "submission_date": "2019-03-15T11:46:28+00:00",
            "submitted_by": "yibin",
            "description": "Embeddings discovered by the VAE model, we use biLSTM in encoder, and LSTM in decoder. Specifically, we implement Speaker_Verification system to remove the personal voice feature, so that we can concentrate on the unit embedding for target language",
            "publication": {
                "author_short": "Horizon Robotics",
                "authors": "-",
                "paper_title": "-",
                "paper_ref": "- (2019.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Horizon Robotics(https://www.horizon.ai/)",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.97,
                    "cer": 0.88,
                    "similarity": 1.55,
                    "abx": 30.5,
                    "bitrate": 832.58
                },
                "austronesian": {
                    "mos": 3.55,
                    "cer": 0.32,
                    "similarity": 1.34,
                    "abx": 24.92,
                    "bitrate": 842.46
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315114628_yibin.wav",
                        "sample_2": "english_2_20190315114628_yibin.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315114628_yibin.wav",
                        "sample_2": "surprise_2_20190315114628_yibin.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 30.5,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 44.32
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 24.92,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 47.17
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 832.58
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 842.46
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "G\u00f6k19",
            "submission_id": "20190316092256_murat.saraclar",
            "index": 17,
            "submission_date": "2019-03-16T09:22:56+00:00",
            "submitted_by": "murat.saraclar",
            "description": "A recurrent sparse autoencoder is trained to obtain an alternative and compressible frame-level representation. This representation is obtained through an intermediate feed-forward softmax layer, using the hidden state activations of the network at each timestep, yielding a posteriorgram-like structure. This network is then fine tuned in similar fashion to the correspondence autoencoder [1], with pairs of acoustic segments belonging to the same cluster. These pairs are obtained using an unsupervised term discovery system [2], and the frames of each pair are aligned through dynamic time warping. The intermediate representation is penalized with negative L2-norm during training, to force the representation to become closer to one-hot vectors, hence reducing the quantization error when they are eventually converted to one-hot vectors for compression and baseline synthesizer training. The final embeddings are calculated by passing the one-hot vectors through a median filter to discard possibly erroneous states lasting only one or two frames and enforce continuity, then removing the repetitions. [1] H. Kamper, Truly unsupervised acoustic word embeddings using weak top-down constraints in encoder-decoder models. arXiv preprint arXiv 1811.00403, 2018 [2] A. Jansen and B. Van Durme, Efficient spoken term discovery using randomized algorithms, in Proc. ASRU, 2011",
            "publication": {
                "author_short": "Gok <i>et al.</i>",
                "authors": "Yusuf, B., G\u00f6k, A., G\u00fcndogdu, B., Kose, O. & Saraclar, M. ",
                "paper_title": "Temporally-aware acoustic unit discovery for zerospeech 2019 challenge.",
                "paper_ref": "Yusuf, B., G\u00f6k, A., G\u00fcndogdu, B., Kose, O. & Saraclar, M.  (2019.0) Temporally-aware acoustic unit discovery for zerospeech 2019 challenge.",
                "bib_ref": "yusuf2019hierarchical",
                "paper_url": "https://www.researchgate.net/publication/335830054_Temporally-Aware_Acoustic_Unit_Discovery_for_Zerospeech_2019_Challenge",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Bogazici University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.02,
                    "cer": 0.9,
                    "similarity": 2.93,
                    "abx": 35.86,
                    "bitrate": 34.66
                },
                "austronesian": {
                    "mos": 1.46,
                    "cer": 0.86,
                    "similarity": 3.03,
                    "abx": 27.26,
                    "bitrate": 29.46
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190316092256_murat.saraclar.wav",
                        "sample_2": "english_2_20190316092256_murat.saraclar.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190316092256_murat.saraclar.wav",
                        "sample_2": "surprise_2_20190316092256_murat.saraclar.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 30.35,
                            "auxiliary1_abx_dtw_kl": 30.61,
                            "auxiliary1_abx_levenshtein": 28.94,
                            "auxiliary2_abx_dtw_cosine": 18.57,
                            "auxiliary2_abx_dtw_kl": 20.72,
                            "auxiliary2_abx_levenshtein": 44.3,
                            "test_abx_dtw_cosine": 35.86,
                            "test_abx_dtw_kl": 36.87,
                            "test_abx_levenshtein": 36.78
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 22.21,
                            "auxiliary1_abx_dtw_kl": 22.82,
                            "auxiliary1_abx_levenshtein": 23.43,
                            "auxiliary2_abx_dtw_cosine": 10.87,
                            "auxiliary2_abx_dtw_kl": 12.46,
                            "auxiliary2_abx_levenshtein": 47.1,
                            "test_abx_dtw_cosine": 27.26,
                            "test_abx_dtw_kl": 27.17,
                            "test_abx_levenshtein": 30.28
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 275.85,
                            "auxiliary2_bitrate": 1216.31,
                            "test_bitrate": 34.66
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 279.48,
                            "auxiliary2_bitrate": 1732.67,
                            "test_bitrate": 29.46
                        }
                    }
                },
                "extra_description": [
                    "Frame-level softmax outputs, converted to one-hot vectors.",
                    "Frame-level softmax outputs (unquantized) from the encoder RNN."
                ]
            }
        },
        {
            "model_id": "Ral19",
            "submission_id": "20190316080728_ywt52",
            "index": 15,
            "submission_date": "2019-03-16T08:07:28+00:00",
            "submitted_by": "ywt52",
            "description": "DPGMM + Merlin",
            "publication": {
                "author_short": "Rallabandi <i>et al.</i>",
                "authors": "Sai Krishna Rallabandi, Wenting Ye, Steven Hillis, Elizabeth Salesky, Alan W. Black",
                "paper_title": "-",
                "paper_ref": "Sai Krishna Rallabandi, Wenting Ye, Steven Hillis, Elizabeth Salesky, Alan W. Black (2019.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Carnegie Mellon University",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.02,
                    "cer": 0.82,
                    "similarity": 2.88,
                    "abx": 39.31,
                    "bitrate": 58.19
                },
                "austronesian": {
                    "mos": 1.89,
                    "cer": 0.71,
                    "similarity": 3.02,
                    "abx": 28.41,
                    "bitrate": 71.42
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190316080728_ywt52.wav",
                        "sample_2": "english_2_20190316080728_ywt52.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190316080728_ywt52.wav",
                        "sample_2": "surprise_2_20190316080728_ywt52.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 38.24,
                            "auxiliary1_abx_dtw_kl": 37.45,
                            "auxiliary1_abx_levenshtein": 35.62,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 39.31,
                            "test_abx_dtw_kl": 39.02,
                            "test_abx_levenshtein": 38.09
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 27.44,
                            "auxiliary1_abx_dtw_kl": 27.5,
                            "auxiliary1_abx_levenshtein": 29.45,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 28.41,
                            "test_abx_dtw_kl": 28.54,
                            "test_abx_levenshtein": 29.9
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 539.51,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 58.19
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 546.17,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 71.42
                        }
                    }
                },
                "extra_description": [
                    "None",
                    "None"
                ]
            }
        },
        {
            "model_id": "Kam19b",
            "submission_id": "20190315083546_kamperh",
            "index": 7,
            "submission_date": "2019-03-15T08:35:46+00:00",
            "submitted_by": "kamperh",
            "description": "A neural network model with intermediate discritisation is trained to produce speech features (filterbanks) as output given MFCC features as input. The output is used to condition a trained neural vocoder (FFTNet). Internal label, ryan_conv_deconv_compression_4.",
            "publication": {
                "author_short": "Kamper <i>et al.</i>",
                "authors": "Eloff, R., Nortje, A., Niekerk, B., Govender, A., Nortje, L., Pretorius, A., Van Biljon, E., Westhuizen, E., Staden, L. & Kamper, H.",
                "paper_title": "Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.",
                "paper_ref": "Eloff, R., Nortje, A., Niekerk, B., Govender, A., Nortje, L., Pretorius, A., Van Biljon, E., Westhuizen, E., Staden, L. & Kamper, H. (2019.0) Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.",
                "bib_ref": "eloff2019unsupervised",
                "paper_url": "https://arxiv.org/abs/1904.07557",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Stellenbosch University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.18,
                    "cer": 0.67,
                    "similarity": 2.51,
                    "abx": 27.64,
                    "bitrate": 172.99
                },
                "austronesian": {
                    "mos": 1.96,
                    "cer": 0.6,
                    "similarity": 1.76,
                    "abx": 19.76,
                    "bitrate": 139.54
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315083546_kamperh.wav",
                        "sample_2": "english_2_20190315083546_kamperh.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315083546_kamperh.wav",
                        "sample_2": "surprise_2_20190315083546_kamperh.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 24.29,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 44.86,
                            "auxiliary2_abx_dtw_cosine": 22.96,
                            "auxiliary2_abx_dtw_kl": 24.5,
                            "auxiliary2_abx_levenshtein": 44.26,
                            "test_abx_dtw_cosine": 27.64,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 39.52
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 16.6,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 46.92,
                            "auxiliary2_abx_dtw_cosine": 14.5,
                            "auxiliary2_abx_dtw_kl": 28.25,
                            "auxiliary2_abx_levenshtein": 47.05,
                            "test_abx_dtw_cosine": 19.76,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 36.61
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 399.87,
                            "auxiliary2_bitrate": 1739.85,
                            "test_bitrate": 172.99
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 395.76,
                            "auxiliary2_bitrate": 1745.79,
                            "test_bitrate": 139.54
                        }
                    }
                },
                "extra_description": [
                    "Intermediate continuous features prior to discretisation.",
                    "Intermediate continuous features prior to vocoding but after<br>discretisation."
                ]
            }
        },
        {
            "model_id": "Cho19a",
            "submission_id": "20190315234010_YeonjungHong",
            "index": 14,
            "submission_date": "2019-03-15T23:40:10+00:00",
            "submitted_by": "YeonjungHong",
            "description": "An end-to-end multi-target voice conversion system is submitted from this group. Specifically, following the model proposed by Chorowski et al.(2019), we used Vector Quantized-Variational AutoEncoder (VQ-VAE) for unsupervised speech representation learning, and WaveNet decoder for speech synthesis. We further enhanced the speech representation learning with the speaker-adversarial approach of Chou et al. (2018). However, as this is currently in the early stage of training, here we only submit our baseline inference results without the additional speaker-adversarial training. All of our code is uploaded at https://github.com/Suhee05/Zerospeech2019. Submmited Model Specifications are English (525,000 training steps, mini-batch size 16), Surprise (245,000 training steps, mini-batch size 16).",
            "publication": {
                "author_short": "Cho <i>et al.</i>",
                "authors": "Suhee Cho, Yeonjung Hong, Yookyung Shin, Youngsun Cho, Hyebin Yoon, Hyungwon Yang, Ingu Lee, Seohyun Kim, Wiback Kim, Youngjun Kim, Hosung Nam",
                "paper_title": "-",
                "paper_ref": "Suhee Cho, Yeonjung Hong, Yookyung Shin, Youngsun Cho, Hyebin Yoon, Hyungwon Yang, Ingu Lee, Seohyun Kim, Wiback Kim, Youngjun Kim, Hosung Nam (2019.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Korea University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.91,
                    "cer": 0.64,
                    "similarity": 2.7,
                    "abx": 20.26,
                    "bitrate": 160.88
                },
                "austronesian": {
                    "mos": 1.53,
                    "cer": 0.78,
                    "similarity": 1.33,
                    "abx": 10.39,
                    "bitrate": 144.63
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315234010_YeonjungHong.wav",
                        "sample_2": "english_2_20190315234010_YeonjungHong.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315234010_YeonjungHong.wav",
                        "sample_2": "surprise_2_20190315234010_YeonjungHong.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 20.26,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 37.44
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 10.39,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 32.47
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 160.88
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 144.63
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Tja19b",
            "submission_id": "20190316115857_Sakti",
            "index": 19,
            "submission_date": "2019-03-16T11:58:57+00:00",
            "submitted_by": "Sakti",
            "description": "We proposed VQVAE + Multiscale inverter + GAN enhancement approach. Input -- MFCC features (including deltas) with 39 dimension. VQVAE (To generate embedding) -- 256 codebook, 64 embedding dimension, 4x time reduction. Inverter (To generate speech from codebook) -- multiscale 1D convolution. GAN (To enhance inverter output)",
            "publication": {
                "author_short": "Tjandra <i>et al.</i>",
                "authors": "Tjandra, A., Sakti, S. & Nakamura, S.",
                "paper_title": "Speech-to-speech translation between untranscribed unknown languages.",
                "paper_ref": "Tjandra, A., Sakti, S. & Nakamura, S. (2019.0) Speech-to-speech translation between untranscribed unknown languages.",
                "bib_ref": "tjandra2019speech",
                "paper_url": "https://arxiv.org/abs/1910.00796",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "NAIST (Japan), RIKEN AIP (Japan), NUS (Singapore)",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.66,
                    "cer": 0.37,
                    "similarity": 2.32,
                    "abx": 24.17,
                    "bitrate": 184.32
                },
                "austronesian": {
                    "mos": 3.25,
                    "cer": 0.35,
                    "similarity": 2.67,
                    "abx": 17.8,
                    "bitrate": 151.77
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190316115857_Sakti.wav",
                        "sample_2": "english_2_20190316115857_Sakti.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190316115857_Sakti.wav",
                        "sample_2": "surprise_2_20190316115857_Sakti.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 24.17,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 41.46
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 17.8,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 38.46
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 184.32
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 151.77
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Fen19b",
            "submission_id": "20190315130005_syfeng",
            "index": 11,
            "submission_date": "2019-03-15T13:00:05+00:00",
            "submitted_by": "syfeng",
            "description": "Step 1. Use train/unit/ to train a factorized hierarchical variational auto-encoder (FHVAE) model, with which speaker characteristics information and linguistic content are disentangled into different frame-level representations. Step 2. Use linguistic content representation of train/unit/ learned by FHVAE to train a Dirichlet process GMM (DPGMM) and obtain frame cluster labels. Step 3. Use train/unit/ MFCC features and DPGMM labels and speaker ID labels to train a DNN model with adversarial multi-task learning. Specifically, speaker ID task is set as the adversarial task while DPGMM task is the normal task. Step 4. Extract softmax output layer representation of DPGMM task. This representation is further quantized to one-hot representation by setting the max value inside each frame  to 1, and other values to 0. Consecutive repetitive one-hot features are collapsed. Step 5. Use train/voice/ one-hot representation in Step 4 to perform train speech synthesizer using the given Ossian system. Step 6. Synthesize test/ one-hot representation with the trained synthesizer.",
            "publication": {
                "author_short": "Feng <i>et al.</i>",
                "authors": "Feng, S., Lee, T. & Peng, Z. ",
                "paper_title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.",
                "paper_ref": "Feng, S., Lee, T. & Peng, Z.  (2019.0) Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.",
                "bib_ref": "feng",
                "paper_url": "https://arxiv.org/abs/1906.07235",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "The Chinese University of Hong Kong",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_kl",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 1.56,
                    "cer": 0.84,
                    "similarity": 2.18,
                    "abx": 21.67,
                    "bitrate": 413.23
                },
                "austronesian": {
                    "mos": 1.28,
                    "cer": 0.74,
                    "similarity": 2.01,
                    "abx": 10.64,
                    "bitrate": 470.23
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315130005_syfeng.wav",
                        "sample_2": "english_2_20190315130005_syfeng.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315130005_syfeng.wav",
                        "sample_2": "surprise_2_20190315130005_syfeng.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 13.33,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 44.3,
                            "auxiliary2_abx_dtw_cosine": 13.82,
                            "auxiliary2_abx_dtw_kl": 13.72,
                            "auxiliary2_abx_levenshtein": 44.3,
                            "test_abx_dtw_cosine": 22.32,
                            "test_abx_dtw_kl": 21.67,
                            "test_abx_levenshtein": 26.46
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 6.52,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 47.1,
                            "auxiliary2_abx_dtw_cosine": 7.49,
                            "auxiliary2_abx_dtw_kl": 7.62,
                            "auxiliary2_abx_levenshtein": 47.1,
                            "test_abx_dtw_cosine": 11.44,
                            "test_abx_dtw_kl": 10.64,
                            "test_abx_levenshtein": 18.95
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 1732.81,
                            "auxiliary2_bitrate": 1732.81,
                            "test_bitrate": 413.23
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 1732.67,
                            "auxiliary2_bitrate": 1732.67,
                            "test_bitrate": 470.23
                        }
                    }
                },
                "extra_description": [
                    "Linear bottleneck layer activation representation in DNN model<br>discussed in Step 3 of 'system description'. Frame-wise, no<br>quantization.",
                    "Softmax output layer activation representation in DPGMM task. In log-<br>softmax scale, frame-wise, no quantization."
                ]
            }
        },
        {
            "model_id": "topline19a",
            "submission_id": "20190319093259_mmmaat",
            "index": 21,
            "submission_date": "2019-03-19T09:32:59+00:00",
            "submitted_by": "mmmaat",
            "description": "ASR system (Kaldi) piped to a trained TTS system (Merlin), both trained using supervision (on gold labels).",
            "publication": {
                "author_short": "<b>Topline</b>",
                "authors": null,
                "paper_title": null,
                "paper_ref": null,
                "bib_ref": null,
                "paper_url": null,
                "pub_year": null,
                "team_name": null,
                "institution": "PSL University, CNRS, ENS, EHESS, INRIA",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.77,
                    "cer": 0.44,
                    "similarity": 2.99,
                    "abx": 29.85,
                    "bitrate": 37.73
                },
                "austronesian": {
                    "mos": 3.92,
                    "cer": 0.28,
                    "similarity": 3.95,
                    "abx": 16.09,
                    "bitrate": 35.2
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190319093259_mmmaat.wav",
                        "sample_2": "english_2_20190319093259_mmmaat.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190319093259_mmmaat.wav",
                        "sample_2": "surprise_2_20190319093259_mmmaat.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 29.85,
                            "test_abx_dtw_kl": 30.28,
                            "test_abx_levenshtein": 29.41
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 16.09,
                            "test_abx_dtw_kl": 15.62,
                            "test_abx_levenshtein": 19.14
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 37.73
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 35.2
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Pan19b",
            "submission_id": "20190314042652_pandia",
            "index": 2,
            "submission_date": "2019-03-14T04:26:52+00:00",
            "submitted_by": "pandia",
            "description": "Speech units of the form CV, VC transients and steady-state regions are modelled using GMM-HMM",
            "publication": {
                "author_short": "Pandia <i>et al.</i>",
                "authors": "Pandia, K. & Murthy, H.",
                "paper_title": "Zero resource speech synthesis using transcripts derived from perceptual acoustic units.",
                "paper_ref": "Pandia, K. & Murthy, H. (2020.0) Zero resource speech synthesis using transcripts derived from perceptual acoustic units.",
                "bib_ref": "murthy2020zero",
                "paper_url": "https://arxiv.org/abs/2006.04373",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Indian Institute of Technology Madras",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.82,
                    "cer": 0.55,
                    "similarity": 2.76,
                    "abx": 29.66,
                    "bitrate": 138.59
                },
                "austronesian": {
                    "mos": 2.53,
                    "cer": 0.43,
                    "similarity": 3.58,
                    "abx": 23.56,
                    "bitrate": 115.43
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190314042652_pandia.wav",
                        "sample_2": "english_2_20190314042652_pandia.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190314042652_pandia.wav",
                        "sample_2": "surprise_2_20190314042652_pandia.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 29.66,
                            "test_abx_dtw_kl": 28.75,
                            "test_abx_levenshtein": 29.91
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 23.56,
                            "test_abx_dtw_kl": 23.92,
                            "test_abx_levenshtein": 25.51
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 138.59
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 115.43
                        }
                    }
                },
                "extra_description": [
                    "-",
                    "-"
                ]
            }
        },
        {
            "model_id": "Cho19b",
            "submission_id": "20190315175746_YeonjungHong",
            "index": 13,
            "submission_date": "2019-03-15T17:57:46+00:00",
            "submitted_by": "YeonjungHong",
            "description": "An end-to-end multi-target voice conversion system is submitted from this group. Specifically, following the model proposed by Chorowski et al.(2019), we used Vector Quantized-Variational AutoEncoder (VQ-VAE) for unsupervised speech representation learning, and WaveNet decoder for speech synthesis. We further enhanced the speech representation learning with the speaker-adversarial approach of Chou et al. (2018). However, as this is currently in the early stage of training, here we only submit our baseline inference results without the additional speaker-adversarial training. All of our code is uploaded at https://github.com/Suhee05/Zerospeech2019. Submmited Model Specifications are English (502,500 training steps, mini-batch size 16), Surprise (222,500 training steps, mini-batch size 16).",
            "publication": {
                "author_short": "Cho <i>et al.</i>",
                "authors": "Suhee Cho, Yeonjung Hong, Yookyung Shin, Youngsun Cho, Hyebin Yoon, Hyungwon Yang, Ingu Lee, Seohyun Kim, Wiback Kim, Youngjun Kim, Hosung Nam",
                "paper_title": "-",
                "paper_ref": "Suhee Cho, Yeonjung Hong, Yookyung Shin, Youngsun Cho, Hyebin Yoon, Hyungwon Yang, Ingu Lee, Seohyun Kim, Wiback Kim, Youngjun Kim, Hosung Nam (2019.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Korea University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.94,
                    "cer": 0.62,
                    "similarity": 2.66,
                    "abx": 20.25,
                    "bitrate": 158.47
                },
                "austronesian": {
                    "mos": 1.23,
                    "cer": 0.85,
                    "similarity": 1.28,
                    "abx": 12.05,
                    "bitrate": 143.76
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190315175746_YeonjungHong.wav",
                        "sample_2": "english_2_20190315175746_YeonjungHong.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190315175746_YeonjungHong.wav",
                        "sample_2": "surprise_2_20190315175746_YeonjungHong.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 20.25,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 37.31
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 12.05,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 34.43
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 158.47
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 143.76
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Yus19",
            "submission_id": "20190316091850_murat.saraclar",
            "index": 16,
            "submission_date": "2019-03-16T09:18:50+00:00",
            "submitted_by": "murat.saraclar",
            "description": "We train a temporal-proximity-aware self-organizing map to discover units from PLP features. Since the SOM units are spatially related, we pool consecutive units and then train a GRU to predict each pooled unit from the PLP features. The test embeddings are obtained by using a hard max on the output (softmax) of the GRU, followed by removing of repeated labels. We use the baseline (Ossian) system for synthesis.",
            "publication": {
                "author_short": "Yusuf <i>et al.</i>",
                "authors": "Yusuf, B., G\u00f6k, A., G\u00fcndogdu, B., Kose, O. & Saraclar, M. ",
                "paper_title": "Temporally-aware acoustic unit discovery for zerospeech 2019 challenge.",
                "paper_ref": "Yusuf, B., G\u00f6k, A., G\u00fcndogdu, B., Kose, O. & Saraclar, M.  (2019.0) Temporally-aware acoustic unit discovery for zerospeech 2019 challenge.",
                "bib_ref": "yusuf2019temporally",
                "paper_url": "https://www.researchgate.net/publication/335830054_Temporally-Aware_Acoustic_Unit_Discovery_for_Zerospeech_2019_Challenge",
                "pub_year": 2019.0,
                "team_name": null,
                "institution": "Bogazici University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false
                }
            },
            "scores": {
                "english": {
                    "mos": 2.42,
                    "cer": 0.86,
                    "similarity": 2.88,
                    "abx": 25.69,
                    "bitrate": 92.37
                },
                "austronesian": {
                    "mos": 1.84,
                    "cer": 0.8,
                    "similarity": 2.84,
                    "abx": 24.16,
                    "bitrate": 121.03
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190316091850_murat.saraclar.wav",
                        "sample_2": "english_2_20190316091850_murat.saraclar.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190316091850_murat.saraclar.wav",
                        "sample_2": "surprise_2_20190316091850_murat.saraclar.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 18.92,
                            "auxiliary1_abx_dtw_kl": 21.06,
                            "auxiliary1_abx_levenshtein": 44.3,
                            "auxiliary2_abx_dtw_cosine": 28.37,
                            "auxiliary2_abx_dtw_kl": 28.26,
                            "auxiliary2_abx_levenshtein": 30.24,
                            "test_abx_dtw_cosine": 25.69,
                            "test_abx_dtw_kl": 25.95,
                            "test_abx_levenshtein": 28.7
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 12.29,
                            "auxiliary1_abx_dtw_kl": 12.92,
                            "auxiliary1_abx_levenshtein": 47.1,
                            "auxiliary2_abx_dtw_cosine": 24.46,
                            "auxiliary2_abx_dtw_kl": 24.36,
                            "auxiliary2_abx_levenshtein": 24.96,
                            "test_abx_dtw_cosine": 24.16,
                            "test_abx_dtw_kl": 24.07,
                            "test_abx_levenshtein": 26.64
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 1216.31,
                            "auxiliary2_bitrate": 49.97,
                            "test_bitrate": 92.37
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 1732.67,
                            "auxiliary2_bitrate": 66.86,
                            "test_bitrate": 121.03
                        }
                    }
                },
                "extra_description": [
                    "We represent each frame as a function of the distance between the PLP<br>features and each SOM unit. The distances are softmax-ed to obtain a<br>probability distribution which is used as the embedding.",
                    "We train the GRU with fewer initial SOM units and obtain the quantized<br>(hard max) outputs."
                ]
            }
        },
        {
            "model_id": "Lum20a",
            "submission_id": "20200425114629_patricklt",
            "index": 22,
            "submission_date": "2020-04-25T11:46:29+00:00",
            "submitted_by": "patricklt",
            "description": "Spectral and excitation model is developed with CycleVAE, which is based on cyclic variational autoencoder (CycleVAE) [https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2307.pdf]. The difference to the above paper is that it estimates excitation parameters with the first half of the first cycle. Also, the difference between the other submission is the use of continuous latent distribution and the joint modeling of spectral and excitation parameters. On the other hand. waveform model is developed with Shallow WaveNet vocoder using softmax output [https://ieeexplore.ieee.org/abstract/document/9003800]. Waveform model is without fine-tuning, unlike the other submission, due to not enough time after spectral-excitation model was trained.",
            "publication": {
                "author_short": "Lumban Tobing <i>et al.</i>",
                "authors": "Tobing, P., Hayashi, T., Wu, Y., Kobayashi, K. & Toda, T. ",
                "paper_title": "Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling.",
                "paper_ref": "Tobing, P., Hayashi, T., Wu, Y., Kobayashi, K. & Toda, T.  (2020.0) Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling.",
                "bib_ref": "tobing2020cyclic",
                "paper_url": "https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2559.pdf",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Nagoya University",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "Speech features WORLD-based 50-dimensional mel-cepstrum including 0th power log-continuous F0 with binary U/V decision and code-aperiodicity 10 ms frame shift 1024 FFT points analysis For CycleVAE joint spectral and excitation modeling 32 latent-dimensions 3 cycles one-hot speaker code is used for all 215 speakers of 2017 datasets -3/+3 contexts for input convolution extraction 1 hidden layer and 1024 hidden units for GRU encoder and spectral decoder 1 hidden layer and 512 hidden units for GRU excitation decoder 0.5 dropout encoder output layer generates latent-posterior with standard Laplacian prior and posterior-probability of speaker-input code spectral decoder input layer receives sampled latent parameters in training, but mode of posterior in decoding, and one-hot code of generated speaker spectral decoder output layer generates mel-cepstrum excitation decoder input layer receives estimated mel-cepstrum and one-hot code of generated speaker excitation decoder output layer generates log-F0, binary U/V decision, and code-aperiodicity loss values including reconstruction mel-cepstrum of all cycles path + KL-divergence to standard Laplacian prior in each cycle path + cross-entropy of speaker-posterior + F0,U/V,code-ap reconstruction loss of 1st half of 1st cycle For WaveNet waveform modeling 256 hidden-channels and skip channels 3 dilation depth (dilation factors with respect to power of kernel-size), 2 dilation repeat 7 kernel-size -3/+3 contexts for input convolution extraction of conditioning features (WORLD-based) upsampling layer with transposed 2d convolution to match 10 ms frame shift output is 256-dimensional categorical values for mu-law discretized waveform samples noise-shaping method is used to obtain target waveform with flatter spectral tilt the multispeaker model (3 targets) was used without fine-tuning (the same multispeaker model from the other submission)"
                }
            },
            "scores": {
                "english": {
                    "mos": 3.31,
                    "cer": 0.31,
                    "similarity": 3.16,
                    "abx": 36.29,
                    "bitrate": 1739.85
                },
                "austronesian": {
                    "mos": 3.84,
                    "cer": 0.21,
                    "similarity": 3.91,
                    "abx": 24.42,
                    "bitrate": 1745.79
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200425114629_patricklt.wav",
                        "sample_2": "english_2_20200425114629_patricklt.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200425114629_patricklt.wav",
                        "sample_2": "surprise_2_20200425114629_patricklt.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 36.29,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 44.26
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 24.42,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 47.05
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 1739.85
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 1745.79
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Hou20a",
            "submission_id": "20200423163548_houwenxin0520",
            "index": 7,
            "submission_date": "2020-04-23T16:35:48+00:00",
            "submitted_by": "houwenxin0520",
            "description": "We propose a system composed of a multi-scale hierarchical VQ-VAE encoder to discover discrete spoken word units from speech and a MelGAN vocoder to directly genenate speech. They are trained separately. During VQ-VAE training, we add the speaker id before the VQ-VAE decoder to help reduce the speaker information encoded in the word units. The difference between this and the other submission is that we employ two auxillary losses --> encoding error loss and reconstruction error loss, to help MelGAN improve synthesis accuracy. Code will be released at https://github.com/houwenxin/zerospeech2020",
            "publication": {
                "author_short": "Hou <i>et al.</i>",
                "authors": "Wenxin Hou, Mingxin Zhang, Shengzhou Gao, Takahiro Shinozaki",
                "paper_title": "-",
                "paper_ref": "Wenxin Hou, Mingxin Zhang, Shengzhou Gao, Takahiro Shinozaki (2020.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Tokyo Institute of Technology",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "VQ-VAE --> learning rate 0.0004, batch size 64, codebook size 128, embedding dim 64, commitment loss weight 0.25; MelGAN --> learning rate 0.0002, batch size 16, downsample factor 4, adversarial loss weight 0.2, feature matching loss weight 0.5, encoding error weight 1.0, reconstruct error weight 0.5;"
                }
            },
            "scores": {
                "english": {
                    "mos": 2.48,
                    "cer": 0.79,
                    "similarity": 3.18,
                    "abx": 42.52,
                    "bitrate": 245.2
                },
                "austronesian": {
                    "mos": 2.16,
                    "cer": 0.75,
                    "similarity": 2.84,
                    "abx": 45.13,
                    "bitrate": 329.23
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200423163548_houwenxin0520.wav",
                        "sample_2": "english_2_20200423163548_houwenxin0520.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200423163548_houwenxin0520.wav",
                        "sample_2": "surprise_2_20200423163548_houwenxin0520.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 43.57,
                            "test_abx_dtw_kl": 43.49,
                            "test_abx_levenshtein": 42.52
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 45.89,
                            "test_abx_dtw_kl": 45.7,
                            "test_abx_levenshtein": 45.13
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 245.2
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 329.23
                        }
                    }
                },
                "extra_description": [
                    "description of the auxiliary1 embeddings (if used)",
                    "description of the auxiliary1 embeddings (if used)"
                ]
            }
        },
        {
            "model_id": "Che20a",
            "submission_id": "20200423182652_mingjiechen",
            "index": 8,
            "submission_date": "2020-04-23T18:26:52+00:00",
            "submitted_by": "mingjiechen",
            "description": "Followed by Chorowski et al.(2019), wavenet autoencoder is used. Instance Normalization layers are used in encoder. Adaptive Instance Normalization is used in decoder. Frame rate is 25 hz, dimension is 64. Speaker encoder is used to encode speaker information. English data is trained for 610k steps and surprise language data is trained for 300k steps. Batch size is 10.",
            "publication": {
                "author_short": "Chen <i>et al.</i>",
                "authors": "Chen, M. & Hain, T. ",
                "paper_title": "Unsupervised acoustic unit representation learning for voice conversion using wavenet auto-encoders.",
                "paper_ref": "Chen, M. & Hain, T.  (2020.0) Unsupervised acoustic unit representation learning for voice conversion using wavenet auto-encoders.",
                "bib_ref": "chen2020unsupervised",
                "paper_url": "https://arxiv.org/abs/2008.06892",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "University of Sheffield",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": {
                        "dimension": 64,
                        "frame rate": 25
                    }
                }
            },
            "scores": {
                "english": {
                    "mos": 3.61,
                    "cer": 0.18,
                    "similarity": 2.57,
                    "abx": 20.19,
                    "bitrate": 385.75
                },
                "austronesian": {
                    "mos": 4.06,
                    "cer": 0.15,
                    "similarity": 2.67,
                    "abx": 12.5,
                    "bitrate": 387.83
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200423182652_mingjiechen.wav",
                        "sample_2": "english_2_20200423182652_mingjiechen.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200423182652_mingjiechen.wav",
                        "sample_2": "surprise_2_20200423182652_mingjiechen.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 20.19,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 44.9
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 12.5,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 46.92
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 385.75
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 387.83
                        }
                    }
                },
                "extra_description": [
                    "",
                    ""
                ]
            }
        },
        {
            "model_id": "Hou20b",
            "submission_id": "20200423163457_houwenxin0520",
            "index": 6,
            "submission_date": "2020-04-23T16:34:57+00:00",
            "submitted_by": "houwenxin0520",
            "description": "We propose a system composed of a multi-scale hierarchical VQ-VAE encoder to discover discrete spoken word units from speech and a MelGAN vocoder to directly genenate speech. They are trained separately. During VQ-VAE training, we add the speaker id before the VQ-VAE decoder to help reduce the speaker information encoded in the word units. Code will be released at https://github.com/houwenxin/zerospeech2020",
            "publication": {
                "author_short": "Hou <i>et al.</i>",
                "authors": "Wenxin Hou, Mingxin Zhang, Shengzhou Gao, Takahiro Shinozaki",
                "paper_title": "-",
                "paper_ref": "Wenxin Hou, Mingxin Zhang, Shengzhou Gao, Takahiro Shinozaki (2020.0) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Tokyo Institute of Technology",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "VQ-VAE --> learning rate 0.0004, batch size 64, codebook size 128, embedding dim 64, commitment loss weight 0.25; MelGAN --> learning rate 0.0002, batch size 16, downsample factor 4, adversarial loss weight 1, feature matching loss weight 10;"
                }
            },
            "scores": {
                "english": {
                    "mos": 2.5,
                    "cer": 0.81,
                    "similarity": 3.17,
                    "abx": 42.52,
                    "bitrate": 245.2
                },
                "austronesian": {
                    "mos": 2.05,
                    "cer": 0.72,
                    "similarity": 2.59,
                    "abx": 45.13,
                    "bitrate": 329.23
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200423163457_houwenxin0520.wav",
                        "sample_2": "english_2_20200423163457_houwenxin0520.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200423163457_houwenxin0520.wav",
                        "sample_2": "surprise_2_20200423163457_houwenxin0520.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 43.57,
                            "test_abx_dtw_kl": 43.49,
                            "test_abx_levenshtein": 42.52
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 45.89,
                            "test_abx_dtw_kl": 45.7,
                            "test_abx_levenshtein": 45.13
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 245.2
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 329.23
                        }
                    }
                },
                "extra_description": [
                    "description of the auxiliary1 embeddings (if used)",
                    "description of the auxiliary1 embeddings (if used)"
                ]
            }
        },
        {
            "model_id": "Che20b",
            "submission_id": "20200423184651_mingjiechen",
            "index": 9,
            "submission_date": "2020-04-23T18:46:51+00:00",
            "submitted_by": "mingjiechen",
            "description": "Followed by Chorowski et al.(2019), wavenet autoencoder is used. Decomposed Vector Quantization is used. Frame rate is 25 hz, dimension is 64. English data is trained for 350k steps and surprise language data is trained for 200k steps. Batch size is 10.",
            "publication": {
                "author_short": "Chen <i>et al.</i>",
                "authors": "Chen, M. & Hain, T. ",
                "paper_title": "Unsupervised acoustic unit representation learning for voice conversion using wavenet auto-encoders.",
                "paper_ref": "Chen, M. & Hain, T.  (2020.0) Unsupervised acoustic unit representation learning for voice conversion using wavenet auto-encoders.",
                "bib_ref": "chen2020unsupervised",
                "paper_url": "https://arxiv.org/abs/2008.06893",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "University of Sheffield",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": {
                        "dimension": 128,
                        "frame rate": 25
                    }
                }
            },
            "scores": {
                "english": {
                    "mos": 2.88,
                    "cer": 0.47,
                    "similarity": 2.35,
                    "abx": 26.06,
                    "bitrate": 377.05
                },
                "austronesian": {
                    "mos": 2.28,
                    "cer": 0.55,
                    "similarity": 2.5,
                    "abx": 16.47,
                    "bitrate": 384.23
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200423184651_mingjiechen.wav",
                        "sample_2": "english_2_20200423184651_mingjiechen.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200423184651_mingjiechen.wav",
                        "sample_2": "surprise_2_20200423184651_mingjiechen.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 26.06,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 44.9
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 16.47,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 46.92
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 377.05
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 384.23
                        }
                    }
                },
                "extra_description": [
                    "",
                    ""
                ]
            }
        },
        {
            "model_id": "Kum20a",
            "submission_id": "20200424134350_ManoRanjithK",
            "index": 13,
            "submission_date": "2020-04-24T13:43:50+00:00",
            "submitted_by": "ManoRanjithK",
            "description": "AUD - Speech units of the form CV, VC transients and steady-state regions are modelled using GMM-HMM ( same as system 1) TTS - Espnet (X-vector) using aud+voice data and waveglow",
            "publication": {
                "author_short": "Kumar <i>et al.</i>",
                "authors": "Prakash, A., Kumar, M., Murthy, H.",
                "paper_title": "Exploration of end-to-end synthesisers for zero resource speech challenge 2020.",
                "paper_ref": "Prakash, A., Kumar, M., Murthy, H. (2020.0) Exploration of end-to-end synthesisers for zero resource speech challenge 2020.",
                "bib_ref": "prakash2020exploration",
                "paper_url": "https://arxiv.org/abs/2009.04983",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Indian Institute of Technology Madras",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "K for KNN-graph - 8 Window scale factor for syllable-like segmentation - 8 Number of iteration for self-training - 15"
                }
            },
            "scores": {
                "english": {
                    "mos": 3.19,
                    "cer": 0.45,
                    "similarity": 2.68,
                    "abx": 33.28,
                    "bitrate": 126.41
                },
                "austronesian": {
                    "mos": 3.66,
                    "cer": 0.44,
                    "similarity": 2.49,
                    "abx": 34.33,
                    "bitrate": 101.0
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424134350_ManoRanjithK.wav",
                        "sample_2": "english_2_20200424134350_ManoRanjithK.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424134350_ManoRanjithK.wav",
                        "sample_2": "surprise_2_20200424134350_ManoRanjithK.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 32.84,
                            "auxiliary1_abx_dtw_kl": 33.07,
                            "auxiliary1_abx_levenshtein": 32.54,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 33.31,
                            "test_abx_dtw_kl": 33.61,
                            "test_abx_levenshtein": 33.28
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 34.18,
                            "auxiliary1_abx_dtw_kl": 34.32,
                            "auxiliary1_abx_levenshtein": 34.25,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 34.08,
                            "test_abx_dtw_kl": 34.03,
                            "test_abx_levenshtein": 34.33
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 138.01,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 126.41
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 117.27,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 101.0
                        }
                    }
                },
                "extra_description": [
                    "expanded one-hot representation",
                    ""
                ]
            }
        },
        {
            "model_id": "Kum20b",
            "submission_id": "20200424133919_ManoRanjithK",
            "index": 12,
            "submission_date": "2020-04-24T13:39:19+00:00",
            "submitted_by": "ManoRanjithK",
            "description": "AUD - Speech units of the form CV, VC transients and steady-state regions are modelled using GMM-HMM (same as system 2) TTS - Espnet on voice data and waveglow",
            "publication": {
                "author_short": "Kumar <i>et al.</i>",
                "authors": "Prakash, A., Kumar, M., Murthy, H.",
                "paper_title": "Exploration of end-to-end synthesisers for zero resource speech challenge 2020.",
                "paper_ref": "Prakash, A., Kumar, M., Murthy, H. (2020.0) Exploration of end-to-end synthesisers for zero resource speech challenge 2020.",
                "bib_ref": "prakash2020exploration",
                "paper_url": "https://arxiv.org/abs/2009.04984",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Indian Institute of Technology Madras",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "K for KNN-graph - 8 Window scale factor for syllable-like segmentation - 8 Number of iteration for self-training - 15"
                }
            },
            "scores": {
                "english": {
                    "mos": 3.15,
                    "cer": 0.61,
                    "similarity": 2.69,
                    "abx": 33.28,
                    "bitrate": 126.41
                },
                "austronesian": {
                    "mos": 3.09,
                    "cer": 0.67,
                    "similarity": 2.45,
                    "abx": 34.33,
                    "bitrate": 101.0
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424133919_ManoRanjithK.wav",
                        "sample_2": "english_2_20200424133919_ManoRanjithK.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424133919_ManoRanjithK.wav",
                        "sample_2": "surprise_2_20200424133919_ManoRanjithK.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 32.84,
                            "auxiliary1_abx_dtw_kl": 33.07,
                            "auxiliary1_abx_levenshtein": 32.54,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 33.31,
                            "test_abx_dtw_kl": 33.61,
                            "test_abx_levenshtein": 33.28
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 34.18,
                            "auxiliary1_abx_dtw_kl": 34.32,
                            "auxiliary1_abx_levenshtein": 34.25,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 34.08,
                            "test_abx_dtw_kl": 34.03,
                            "test_abx_levenshtein": 34.33
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 138.01,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 126.41
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 117.27,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 101.0
                        }
                    }
                },
                "extra_description": [
                    "expanded one-hot representation",
                    ""
                ]
            }
        },
        {
            "model_id": "topline20a",
            "submission_id": "20190319093259_mmmaat",
            "index": 1,
            "submission_date": "2019-03-19T09:32:59+00:00",
            "submitted_by": "mmmaat",
            "description": "ASR system (Kaldi) piped to a trained TTS system (Merlin), both trained using supervision (on gold labels).",
            "publication": {
                "author_short": "<b>Topline</b>",
                "authors": null,
                "paper_title": null,
                "paper_ref": null,
                "bib_ref": null,
                "paper_url": null,
                "pub_year": null,
                "team_name": null,
                "institution": "PSL University, CNRS, ENS, EHESS, INRIA",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": ""
                }
            },
            "scores": {
                "english": {
                    "mos": 2.52,
                    "cer": 0.43,
                    "similarity": 3.1,
                    "abx": 29.85,
                    "bitrate": 37.73
                },
                "austronesian": {
                    "mos": 3.49,
                    "cer": 0.33,
                    "similarity": 3.77,
                    "abx": 16.09,
                    "bitrate": 35.2
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20190319093259_mmmaat.wav",
                        "sample_2": "english_2_20190319093259_mmmaat.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20190319093259_mmmaat.wav",
                        "sample_2": "surprise_2_20190319093259_mmmaat.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 29.85,
                            "test_abx_dtw_kl": 30.28,
                            "test_abx_levenshtein": 29.41
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 16.09,
                            "test_abx_dtw_kl": 15.62,
                            "test_abx_levenshtein": 19.14
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 37.73
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 35.2
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Nie20a",
            "submission_id": "20200424154827_benjaminvan",
            "index": 15,
            "submission_date": "2020-04-24T15:48:27+00:00",
            "submitted_by": "benjaminvan",
            "description": "To learn a discrete representation of speech we propose vector-quantized contrastive predictive coding. The encoder maps input speech into a sequence of latent vectors with 2x downsampling. Latent vectors are then quantized using a codebook of size 512. Next, an autoregressive model summarises the latent representation (up until time t) into a context vector. Using this context, the model learns to discriminate future frames from negative examples sampled randomly from other utterances. Finally, an RNN based vocoder is trained to generate audio from the discretized representation. Code to train and evaluate the models is available at https://github.com/bshall/VectorQuantizedCPC.",
            "publication": {
                "author_short": "Niekerk <i>et al.</i>",
                "authors": "Niekerk, B., Nortje, L. & Kamper, H.",
                "paper_title": "Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge.",
                "paper_ref": "Niekerk, B., Nortje, L. & Kamper, H. (2020.0) Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge.",
                "bib_ref": "van2020vector",
                "paper_url": "https://arxiv.org/abs/2005.09409",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Stellenbosch University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "All hyperparameters used for training the models can be found in the config folder at https://github.com/bshall/VectorQuantizedCPC. The same hyperparameters are used across languages, i.e. no language-specific hyperparameter tuning is necessary."
                }
            },
            "scores": {
                "english": {
                    "mos": 3.64,
                    "cer": 0.38,
                    "similarity": 3.8,
                    "abx": 13.44,
                    "bitrate": 421.33
                },
                "austronesian": {
                    "mos": 3.49,
                    "cer": 0.27,
                    "similarity": 2.68,
                    "abx": 5.13,
                    "bitrate": 419.62
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424154827_benjaminvan.wav",
                        "sample_2": "english_2_20200424154827_benjaminvan.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424154827_benjaminvan.wav",
                        "sample_2": "surprise_2_20200424154827_benjaminvan.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 12.47,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 43.91,
                            "auxiliary2_abx_dtw_cosine": 12.29,
                            "auxiliary2_abx_dtw_kl": 50.0,
                            "auxiliary2_abx_levenshtein": 44.3,
                            "test_abx_dtw_cosine": 13.44,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 27.84
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 4.89,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 46.83,
                            "auxiliary2_abx_dtw_cosine": 5.4,
                            "auxiliary2_abx_dtw_kl": 50.0,
                            "auxiliary2_abx_levenshtein": 47.17,
                            "test_abx_dtw_cosine": 5.13,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 21.0
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 817.37,
                            "auxiliary2_bitrate": 817.69,
                            "test_bitrate": 421.33
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 816.64,
                            "auxiliary2_bitrate": 821.68,
                            "test_bitrate": 419.62
                        }
                    }
                },
                "extra_description": [
                    "Continuous context vectors (output of the autoregressive model).",
                    "Continuous features after projecting but prior to quantization."
                ]
            }
        },
        {
            "model_id": "Yus20a",
            "submission_id": "20200424114655_bolajiy",
            "index": 11,
            "submission_date": "2020-04-24T11:46:55+00:00",
            "submitted_by": "bolajiy",
            "description": "The subspace HMM described in https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2224.pdf is used for acoustic unit discovery. The system is a Bayesian HMM-GMM system whose parameters are constrained to dwell on a low-dimensional subspace of the parameter space. The subspace is pretrained on languages (Spanish, Polish, German, French from the Globalphone corpus) and kept constant for the target languages. The 'test' embeddings are the sequence of units as decoded by the HMM.",
            "publication": {
                "author_short": "Yusuf <i>et al.</i>",
                "authors": "Yusuf, B., Ondel, L., Burget, L., \u010cernock\u1ef3, J. & Saraclar, M.",
                "paper_title": "A hierarchical subspace model for language-attuned acoustic unit discovery.",
                "paper_ref": "Yusuf, B., Ondel, L., Burget, L., \u010cernock\u1ef3, J. & Saraclar, M. (2021.0) A hierarchical subspace model for language-attuned acoustic unit discovery.",
                "bib_ref": "yusuf2021hierarchical",
                "paper_url": "https://arxiv.org/abs/2011.03115",
                "pub_year": 2021.0,
                "team_name": null,
                "institution": "Brno University of Technology, Bogazici University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": true,
                    "parallel_train": false,
                    "hyperparameters": "subspace_dimension=100 number_of_units=75"
                }
            },
            "scores": {
                "english": {
                    "mos": 2.12,
                    "cer": 0.76,
                    "similarity": 2.91,
                    "abx": 36.83,
                    "bitrate": 58.98
                },
                "austronesian": {
                    "mos": 2.49,
                    "cer": 0.62,
                    "similarity": 3.43,
                    "abx": 33.66,
                    "bitrate": 65.76
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424114655_bolajiy.wav",
                        "sample_2": "english_2_20200424114655_bolajiy.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424114655_bolajiy.wav",
                        "sample_2": "surprise_2_20200424114655_bolajiy.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 37.32,
                            "test_abx_dtw_kl": 37.8,
                            "test_abx_levenshtein": 36.83
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 33.55,
                            "test_abx_dtw_kl": 33.0,
                            "test_abx_levenshtein": 33.66
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 58.98
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 65.76
                        }
                    }
                },
                "extra_description": [
                    "description of the auxiliary1 embeddings (if used)",
                    "description of the auxiliary1 embeddings (if used)"
                ]
            }
        },
        {
            "model_id": "Lum20b",
            "submission_id": "20200425111550_patricklt",
            "index": 21,
            "submission_date": "2020-04-25T11:15:50+00:00",
            "submitted_by": "patricklt",
            "description": "Spectral model is developed with CycleVQVAE, which is based on cyclic variational autoencoder (CycleVAE) [https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2307.pdf]. The difference is that it uses vector-quantized (discrete) latent-space with straight-through estimation for optimization, instead of the usual continuous latent space in CycleVAE. On the other hand. waveform model is developed with Shallow WaveNet vocoder using softmax output [https://ieeexplore.ieee.org/abstract/document/9003800]. Fine-tuning of waveform model is performed using reconstructed features from spectral model to reduce the mismatches between natural and generated features as in [https://ieeexplore.ieee.org/abstract/document/8913551] Additionaly, an excitation model is also developed with a convolutional and recurrent neural network, which is fed with spectral parameters and speaker code. Hence, to generate waveform, the vq latent features are used along with a speaker code to decode spectral parameters, then excitation parameters, and finally waveform samples.",
            "publication": {
                "author_short": "Lumban Tobing <i>et al.</i>",
                "authors": "Tobing, P., Hayashi, T., Wu, Y., Kobayashi, K. & Toda, T. ",
                "paper_title": "Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling.",
                "paper_ref": "Tobing, P., Hayashi, T., Wu, Y., Kobayashi, K. & Toda, T.  (2020.0) Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling.",
                "bib_ref": "tobing2020cyclic",
                "paper_url": "https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2559.pdf",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Nagoya University",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "Speech features WORLD-based 50-dimensional mel-cepstrum including 0th power log-continuous F0 with binary U/V decision and code-aperiodicity 10 ms frame shift 1024 FFT points analysis For CycleVQVAE spectral modeling 50 centroids 50 latent-dimensions 2 cycles one-hot speaker code is used for all 215 speakers of 2019 dataset -4/+4 contexts for input convolution extraction 1 hidden layer and 1024 hidden units for GRU 0.5 dropout encoder output layer generates estimation of vq embedding and posterior-probability of speaker-input code decoder input layer receives closest vq-embedding and one-hot code of generated speaker decoder output layer generates mel-cepstrum loss values including reconstruction mel-cepstrum of all cycles path + vq-loss of 1st half of 1st cycle + cross-entropy of speaker-posterior For excitation modeling 4 stacks of GRU, 1 hidden unit each, and 1 convolution output layer each Input is added to the output (residual-style), and on the third output GRU, the very first input is multiplied with the outputs of 1st, 2nd, and 3rd modules Finally, 4th module generates U/V decision, log-F0 and code-aperiodicity For WaveNet waveform modeling 256 hidden-channels and skip channels 3 dilation depth (dilation factors with respect to power of kernel-size), 2 dilation repeat 7 kernel-size -4/+4 contexts for input convolution extraction of conditioning features (WORLD-based) upsampling layer with transposed 2d convolution to match 10 ms frame shift output is 256-dimensional categorical values for mu-law discretized waveform samples noise-shaping method is used to obtain target waveform with flatter spectral tilt the multispeaker model (3 targets) was fine-tuned with each target speaker using reconstructed mel-cepstrum from CycleVQVAE, resulting in 3 speaker-dependent models"
                }
            },
            "scores": {
                "english": {
                    "mos": 3.4,
                    "cer": 0.46,
                    "similarity": 3.79,
                    "abx": 30.54,
                    "bitrate": 468.23
                },
                "austronesian": {
                    "mos": 3.28,
                    "cer": 0.33,
                    "similarity": 3.64,
                    "abx": 18.13,
                    "bitrate": 463.75
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200425111550_patricklt.wav",
                        "sample_2": "english_2_20200425111550_patricklt.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200425111550_patricklt.wav",
                        "sample_2": "surprise_2_20200425111550_patricklt.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 30.54,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 34.36
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 18.13,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 28.84
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 468.23
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 463.75
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "G\u00fcn20",
            "submission_id": "20200425093446_murat.saraclar",
            "index": 19,
            "submission_date": "2020-04-25T09:34:46+00:00",
            "submitted_by": "murat.saraclar",
            "description": "The system is based on the Correspondence Recurrent Sparse Autoencoder (CoRSA) model with the following additions. A modified version of winner-take-all networks is applied to the bottleneck layer activations in order to ensure continuity of the activations. This layer is then quantized to the nearest one-hot vector. In addition to the MSE reconstruction loss and the negative L2 sparsity loss, speaker adversarial training is also applied to make the encoder embeddings speaker-independent.",
            "publication": {
                "author_short": "G\u00fcndo\u011fdu <i>et al.</i>",
                "authors": "G\u00fcndogdu, B., Yusuf, B., Yesilbursa, M. & Saraclar, M.",
                "paper_title": "Vector quantized temporally-aware correspondence sparse autoencoders for zero-resource acoustic unit discovery..",
                "paper_ref": "G\u00fcndogdu, B., Yusuf, B., Yesilbursa, M. & Saraclar, M. (2020.0) Vector quantized temporally-aware correspondence sparse autoencoders for zero-resource acoustic unit discovery..",
                "bib_ref": "gundogdu2020vector",
                "paper_url": "https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2765.pdf",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Bo\u011fazi\u00e7i University National Defense University Naval Academy",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_kl",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "learn_rate = 0.001 learn_rate_corsa = 0.0005 batch_size = 1024 batch_size_corsa = 256 num_units = 64 embedding_dim = 128 timesteps = 250 timesteps_corsa = 80 L2-norm_weight = 1.0 adversarial_training_weight = 1.0"
                }
            },
            "scores": {
                "english": {
                    "mos": 1.28,
                    "cer": 0.96,
                    "similarity": 2.16,
                    "abx": 30.05,
                    "bitrate": 34.61
                },
                "austronesian": {
                    "mos": 1.39,
                    "cer": 0.9,
                    "similarity": 2.84,
                    "abx": 28.69,
                    "bitrate": 20.25
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200425093446_murat.saraclar.wav",
                        "sample_2": "english_2_20200425093446_murat.saraclar.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200425093446_murat.saraclar.wav",
                        "sample_2": "surprise_2_20200425093446_murat.saraclar.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 29.93,
                            "test_abx_dtw_kl": 30.05,
                            "test_abx_levenshtein": 32.31
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 28.3,
                            "test_abx_dtw_kl": 28.69,
                            "test_abx_levenshtein": 28.48
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 34.61
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 20.25
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Yus20b",
            "submission_id": "20200424114547_bolajiy",
            "index": 10,
            "submission_date": "2020-04-24T11:45:47+00:00",
            "submitted_by": "bolajiy",
            "description": "The subspace HMM described in https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2224.pdf is used for acoustic unit discovery. The system is a Bayesian HMM-GMM system whose parameters are constrained to dwell on a low-dimensional subspace of the parameter space. The subspace is pretrained on languages (Spanish, Polish, German, French from the Globalphone corpus) and kept constant for the target languages. The 'test' embeddings are the sequence of units as decoded by the HMM.",
            "publication": {
                "author_short": "Yusuf <i>et al.</i>",
                "authors": "Yusuf, B., Ondel, L., Burget, L., \u010cernock\u1ef3, J. & Saraclar, M.",
                "paper_title": "A hierarchical subspace model for language-attuned acoustic unit discovery.",
                "paper_ref": "Yusuf, B., Ondel, L., Burget, L., \u010cernock\u1ef3, J. & Saraclar, M. (2021.0) A hierarchical subspace model for language-attuned acoustic unit discovery.",
                "bib_ref": "yusuf2021hierarchical",
                "paper_url": "https://arxiv.org/abs/2011.031155",
                "pub_year": 2021.0,
                "team_name": null,
                "institution": "Brno University of Technology, Bogazici University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": true,
                    "parallel_train": false,
                    "hyperparameters": "subspace_dimension=100 number_of_units=100"
                }
            },
            "scores": {
                "english": {
                    "mos": 1.99,
                    "cer": 0.75,
                    "similarity": 3.18,
                    "abx": 36.63,
                    "bitrate": 64.16
                },
                "austronesian": {
                    "mos": 2.37,
                    "cer": 0.62,
                    "similarity": 3.14,
                    "abx": 33.99,
                    "bitrate": 72.41
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424114547_bolajiy.wav",
                        "sample_2": "english_2_20200424114547_bolajiy.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424114547_bolajiy.wav",
                        "sample_2": "surprise_2_20200424114547_bolajiy.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 37.51,
                            "test_abx_dtw_kl": 39.3,
                            "test_abx_levenshtein": 36.63
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 34.26,
                            "test_abx_dtw_kl": 35.3,
                            "test_abx_levenshtein": 33.99
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 64.16
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 72.41
                        }
                    }
                },
                "extra_description": [
                    "description of the auxiliary1 embeddings (if used)",
                    "description of the auxiliary1 embeddings (if used)"
                ]
            }
        },
        {
            "model_id": "Tja20a",
            "submission_id": "20200425001923_androstj",
            "index": 18,
            "submission_date": "2020-04-25T00:19:23+00:00",
            "submitted_by": "androstj",
            "description": "Transformers VQVAE",
            "publication": {
                "author_short": "Tjandra <i>et al.</i>",
                "authors": "Tjandra, A., Sakti, S. & Nakamura, S.",
                "paper_title": "Transformer vq-vae for unsupervised unit discovery and speech synthesis: Zerospeech 2020 challenge.",
                "paper_ref": "Tjandra, A., Sakti, S. & Nakamura, S. (2020.0) Transformer vq-vae for unsupervised unit discovery and speech synthesis: Zerospeech 2020 challenge.",
                "bib_ref": "tjandra2020transformer",
                "paper_url": "https://arxiv.org/abs/2005.11676",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "NAIST",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "2 time stride"
                }
            },
            "scores": {
                "english": {
                    "mos": 2.28,
                    "cer": 0.34,
                    "similarity": 1.78,
                    "abx": 24.0,
                    "bitrate": 348.74
                },
                "austronesian": {
                    "mos": 2.72,
                    "cer": 0.28,
                    "similarity": 1.98,
                    "abx": 17.48,
                    "bitrate": 351.49
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200425001923_androstj.wav",
                        "sample_2": "english_2_20200425001923_androstj.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200425001923_androstj.wav",
                        "sample_2": "surprise_2_20200425001923_androstj.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 24.0,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 38.74
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 17.48,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 39.24
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 348.74
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 351.49
                        }
                    }
                },
                "extra_description": [
                    "",
                    ""
                ]
            }
        },
        {
            "model_id": "Liu20",
            "submission_id": "20201125004600_liucodalab",
            "index": 23,
            "submission_date": "2020-11-25T12:46:00+00:00",
            "submitted_by": "liucodalab",
            "description": "Use VQVAE to perform AUD, the encoder of VQVAE is consist of DoConv layers.",
            "publication": {
                "author_short": "Liu*",
                "authors": null,
                "paper_title": null,
                "paper_ref": null,
                "bib_ref": null,
                "paper_url": null,
                "pub_year": null,
                "team_name": null,
                "institution": "Information Engineering University",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": ""
                }
            },
            "scores": {
                "english": {
                    "mos": null,
                    "cer": null,
                    "similarity": null,
                    "abx": 13.35,
                    "bitrate": 408.37
                },
                "austronesian": {
                    "mos": null,
                    "cer": null,
                    "similarity": null,
                    "abx": 6.69,
                    "bitrate": 422.49
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20201125004600_liucodalab.wav",
                        "sample_2": "english_2_20201125004600_liucodalab.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20201125004600_liucodalab.wav",
                        "sample_2": "surprise_2_20201125004600_liucodalab.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 13.35,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 35.2
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 6.69,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 33.62
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 408.37
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 422.49
                        }
                    }
                },
                "extra_description": [
                    "not used",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Nie20b",
            "submission_id": "20200424134758_benjaminvan",
            "index": 14,
            "submission_date": "2020-04-24T13:47:58+00:00",
            "submitted_by": "benjaminvan",
            "description": "An end-to-end vector-quantized variational autoencoder. We use a light-weight decoder based on WaveRNN, allowing us to train on a single GPU. The encoder downsamples input log-Mel spectrograms by a factor of 2 before projecting into a 64 dimensional latent space. Latent vectors are quantized using a codebook of size 512. Code to train and evaluate the models is available at https://github.com/bshall/ZeroSpeech.",
            "publication": {
                "author_short": "Niekerk <i>et al.</i>",
                "authors": "Niekerk, B., Nortje, L. & Kamper, H.",
                "paper_title": "Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge.",
                "paper_ref": "Niekerk, B., Nortje, L. & Kamper, H. (2020.0) Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge.",
                "bib_ref": "van2020vector",
                "paper_url": "https://arxiv.org/abs/2005.09410",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "Stellenbosch University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "All hyperparameters used for training the models can be found in the config folder at https://github.com/bshall/ZeroSpeech. The same hyperparameters are used across languages, i.e. no language-specific hyperparameter tuning is necessary."
                }
            },
            "scores": {
                "english": {
                    "mos": 3.62,
                    "cer": 0.39,
                    "similarity": 3.49,
                    "abx": 14.04,
                    "bitrate": 412.24
                },
                "austronesian": {
                    "mos": 3.71,
                    "cer": 0.21,
                    "similarity": 2.59,
                    "abx": 6.29,
                    "bitrate": 423.67
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424134758_benjaminvan.wav",
                        "sample_2": "english_2_20200424134758_benjaminvan.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424134758_benjaminvan.wav",
                        "sample_2": "surprise_2_20200424134758_benjaminvan.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 13.17,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 44.3,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 14.04,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 35.93
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 8.3,
                            "auxiliary1_abx_dtw_kl": 50.0,
                            "auxiliary1_abx_levenshtein": 47.17,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 6.29,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 32.99
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 815.99,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 412.24
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 817.57,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 423.67
                        }
                    }
                },
                "extra_description": [
                    "Continuous features after projecting but prior to quantization.",
                    ""
                ]
            }
        },
        {
            "model_id": "baseline20a",
            "submission_id": "20200416082658_mmmaat",
            "index": 2,
            "submission_date": "2020-04-16T08:26:58+00:00",
            "submitted_by": "mmmaat",
            "description": "<a href=\"https://zerospeech.com/2019/getting_started.html#baseline-system\">Baseline of ZeroSpeech 2019</a>",
            "publication": {
                "author_short": "<b>Baseline</b>",
                "authors": null,
                "paper_title": null,
                "paper_ref": null,
                "bib_ref": null,
                "paper_url": null,
                "pub_year": null,
                "team_name": null,
                "institution": "PSL University, CNRS, ENS, EHESS, INRIA",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "See <a href=\"https://zerospeech.com/2019/getting_started.html#baseline-system\">Baseline of ZeroSpeech 2019</a>"
                }
            },
            "scores": {
                "english": {
                    "mos": 2.14,
                    "cer": 0.77,
                    "similarity": 2.98,
                    "abx": 35.63,
                    "bitrate": 71.98
                },
                "austronesian": {
                    "mos": 2.23,
                    "cer": 0.67,
                    "similarity": 3.26,
                    "abx": 27.46,
                    "bitrate": 74.55
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200416082658_mmmaat.wav",
                        "sample_2": "english_2_20200416082658_mmmaat.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200416082658_mmmaat.wav",
                        "sample_2": "surprise_2_20200416082658_mmmaat.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 35.63,
                            "test_abx_dtw_kl": 35.97,
                            "test_abx_levenshtein": 34.74
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 27.46,
                            "test_abx_dtw_kl": 28.29,
                            "test_abx_levenshtein": 29.21
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 71.98
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 74.55
                        }
                    }
                },
                "extra_description": [
                    "Embeddings discovered by the Ondel algorithm, before conversion to one-hot vectors",
                    "not used"
                ]
            }
        },
        {
            "model_id": "Tja20b",
            "submission_id": "20200424220530_androstj",
            "index": 17,
            "submission_date": "2020-05-12T11:13:23+00:00",
            "submitted_by": "androstj",
            "description": "This system consisted of two seperated components:  1) VQ-VAE [1] to learn unsupervised discrete codebook representations.  The encoder is consisted of residual convolution 1D (with stride > 1) + Transformers [2].  The decoder is consisted of convolution 1D + upsampling block.  2) Transformer inverter to re-synthesize the linear spectrogram from the codebook representation      The inverter is consisted of several multiscale convolution + Transformers block.   References:       [1] Neural Discrete Representation Learning (Oord et al, NIPS 2017)      [2] Attention is all you need (Vaswani et al, NIPS 2017)",
            "publication": {
                "author_short": "Tjandra <i>et al.</i>*",
                "authors": "Tjandra, A., Sakti, S. & Nakamura, S.",
                "paper_title": "Transformer vq-vae for unsupervised unit discovery and speech synthesis: Zerospeech 2020 challenge.",
                "paper_ref": "Tjandra, A., Sakti, S. & Nakamura, S. (2020.0) Transformer vq-vae for unsupervised unit discovery and speech synthesis: Zerospeech 2020 challenge.",
                "bib_ref": "tjandra2020transformer",
                "paper_url": "https://arxiv.org/abs/2005.11677",
                "pub_year": 2020.0,
                "team_name": null,
                "institution": "NAIST",
                "code": null,
                "DOI": null,
                "open_science": false
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "dtw_cosine",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "4 time stride in the encoder"
                }
            },
            "scores": {
                "english": {
                    "mos": null,
                    "cer": null,
                    "similarity": null,
                    "abx": 20.71,
                    "bitrate": 167.02
                },
                "austronesian": {
                    "mos": null,
                    "cer": null,
                    "similarity": null,
                    "abx": 14.17,
                    "bitrate": 170.92
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200424220530_androstj.wav",
                        "sample_2": "english_2_20200424220530_androstj.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200424220530_androstj.wav",
                        "sample_2": "surprise_2_20200424220530_androstj.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 20.71,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 37.95
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": null,
                            "auxiliary1_abx_dtw_kl": null,
                            "auxiliary1_abx_levenshtein": null,
                            "auxiliary2_abx_dtw_cosine": null,
                            "auxiliary2_abx_dtw_kl": null,
                            "auxiliary2_abx_levenshtein": null,
                            "test_abx_dtw_cosine": 14.17,
                            "test_abx_dtw_kl": 50.0,
                            "test_abx_levenshtein": 37.25
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 167.02
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": null,
                            "auxiliary2_bitrate": null,
                            "test_bitrate": 170.92
                        }
                    }
                },
                "extra_description": [
                    "",
                    ""
                ]
            }
        },
        {
            "model_id": "Mor20",
            "submission_id": "20200425105026_tmorita",
            "index": 20,
            "submission_date": "2020-04-25T10:50:26+00:00",
            "submitted_by": "tmorita",
            "description": "We explored the TTS w/o T task using more biologically/psychologically motivated modules (encoder, discrete VAE, decoder) than popular mechanical networks. The encoder processed 13 MFCCs and their 1st & 2nd derivatives with an echo-state network (ESN), which is said to model computations in cortical microcircuits (Jaeger & Haas, 2004). The encoded features were discretized by our original VAE, named the ABCD-VAE (whose first four letters stand for the Attention-Based Categorical sampling with the Dirichlet prior). The ABCD-VAE combined the the Dirichlet prior and the categorical VAE (sampling approximated by the Gumbel-Softmax; Jang et al., 2017), and implemented the Bayesian clustering, which is popular in computational linguistics/psychology, within the end-to-end system. Thanks to the Dirichlet prior, the system discovered the optimal number of the latent categories (under an arbitrary upper bound, which we set as 256). The decoder was the neural source-filter model proposed by Wang et al. (2020), which we considered to be closer to human's vocal production than WaveNet. The code is available at https://github.com/tkc-morita/ZeroSpeech2020_TTSwoT.",
            "publication": {
                "author_short": "Morita <i>et al.</i>",
                "authors": "Takashi Morita, Hiroki Koda",
                "paper_title": "-",
                "paper_ref": "Takashi Morita, Hiroki Koda (-) -",
                "bib_ref": "-",
                "paper_url": "-",
                "pub_year": "-",
                "team_name": null,
                "institution": "Primate Research Institute, Kyoto University",
                "code": null,
                "DOI": null,
                "open_science": true
            },
            "details": {
                "train_set": null,
                "benchmarks": [
                    "TTS0-19"
                ],
                "gpu_budget": null,
                "parameters": {
                    "abx_distance": "levenshtein",
                    "external_data": false,
                    "parallel_train": false,
                    "hyperparameters": "--iterations 36000 --pretrain_iters 4000 --saving_interval 1000 --milestones 16000 24000 32000 -l 0.0004 --other_hidden_size 128 --feature_dim 128 --esn_hidden_size 2048 --articulatory_channels 64 --num_feature_categories 256 --batch_size 16 --num_workers 16 --max_duration 1.0"
                }
            },
            "scores": {
                "english": {
                    "mos": 1.19,
                    "cer": 0.67,
                    "similarity": 1.14,
                    "abx": 39.3,
                    "bitrate": 137.58
                },
                "austronesian": {
                    "mos": 1.77,
                    "cer": 0.46,
                    "similarity": 1.22,
                    "abx": 34.41,
                    "bitrate": 151.03
                }
            },
            "extras": {
                "audio_samples": {
                    "english": {
                        "sample_1": "english_1_20200425105026_tmorita.wav",
                        "sample_2": "english_2_20200425105026_tmorita.wav"
                    },
                    "austronesian": {
                        "sample_1": "surprise_1_20200425105026_tmorita.wav",
                        "sample_2": "surprise_2_20200425105026_tmorita.wav"
                    }
                },
                "detailed_scores": {
                    "abx": {
                        "english": {
                            "auxiliary1_abx_dtw_cosine": 38.44,
                            "auxiliary1_abx_dtw_kl": 35.46,
                            "auxiliary1_abx_levenshtein": 46.06,
                            "auxiliary2_abx_dtw_cosine": 37.54,
                            "auxiliary2_abx_dtw_kl": 50.0,
                            "auxiliary2_abx_levenshtein": 39.3,
                            "test_abx_dtw_cosine": 40.76,
                            "test_abx_dtw_kl": 40.72,
                            "test_abx_levenshtein": 39.3
                        },
                        "austronesian": {
                            "auxiliary1_abx_dtw_cosine": 27.97,
                            "auxiliary1_abx_dtw_kl": 18.87,
                            "auxiliary1_abx_levenshtein": 45.89,
                            "auxiliary2_abx_dtw_cosine": 27.79,
                            "auxiliary2_abx_dtw_kl": 50.0,
                            "auxiliary2_abx_levenshtein": 34.41,
                            "test_abx_dtw_cosine": 31.81,
                            "test_abx_dtw_kl": 31.41,
                            "test_abx_levenshtein": 34.41
                        }
                    },
                    "bitrate": {
                        "english": {
                            "auxiliary1_bitrate": 405.37,
                            "auxiliary2_bitrate": 137.58,
                            "test_bitrate": 137.58
                        },
                        "austronesian": {
                            "auxiliary1_bitrate": 409.8,
                            "auxiliary2_bitrate": 151.03,
                            "test_bitrate": 151.03
                        }
                    }
                },
                "extra_description": [
                    "The posterior classification probability (best evaluated by dtw_kl).",
                    "The classification logits (best evaluated by dtw_cosine)."
                ]
            }
        }
    ]
}