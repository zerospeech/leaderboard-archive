description: This system combined a baseline method with a Deep Clustering method.
  Specifically, we trained an autoregressive neural network model for phoneme classification
  using pseudo-labels obtained by clustering outputs of a CPC network with k-means,
  and then executing the second-round clustering with the outputs of the final layer
  of the autoregressive model. We used a CPC-big baseline model which was trained
  on Libri-light 6k as a seed model, and used LibriSpeech 960h for pseudo-label phoneme
  classification tasks by constructing the same network as the CPC-big model. We adopted
  a BERT-small model on LibriSpeech 960h for LM training. Experimental results show
  that a relative improvement in the sBLIMP score of 2.3 % is achieved compared to
  a baseline method (CPC-big+km50+BERT-small).
details:
  benchmarks:
  - sLM-21
  - ABX-LS
  gpu_budget: '60'
  parameters:
    frame_shift: 0.01
    metric: cosine
    visually_grounded: false
  train_set: 'CPC training: Libri-light 6k, Phoneme classification task: LibriSpeech
    960h, kmeans: LibriSpeech 100h, LM: LibriSpeech 960h'
extras: null
index: 13
model_id: Mae21a
publication:
  DOI: null
  author_short: Maekaku <i>et al.</i>
  authors: 'Maekaku, T., Chang, X., Fujita, Y., Chen, L., Watanabe, S. & Rudnicky,
    A. '
  bib_ref: maekaku2021speech
  code: null
  institution: Yahoo Japan Corporation, Carnegie Mellon University
  open_science: false
  paper_ref: Maekaku, T., Chang, X., Fujita, Y., Chen, L., Watanabe, S. & Rudnicky,
    A.  (2021.0) Speech representation learning combining conformer cpc with deep
    cluster for the zerospeech challenge 2021.
  paper_title: Speech representation learning combining conformer cpc with deep cluster
    for the zerospeech challenge 2021.
  paper_url: https://arxiv.org/abs/2107.05899
  pub_year: 2021
  team_name: null
scores:
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.0646983906626701
  seed: 3459
  speaker_mode: within
  subset: dev-clean
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.0677141025662422
  seed: 3459
  speaker_mode: across
  subset: dev-clean
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0253517292439937
  seed: 3459
  speaker_mode: across
  subset: dev-clean
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0204001571983099
  seed: 3459
  speaker_mode: within
  subset: dev-clean
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.0921094864606857
  seed: 3459
  speaker_mode: within
  subset: dev-other
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.1028277873992919
  seed: 3459
  speaker_mode: across
  subset: dev-other
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0661970973014831
  seed: 3459
  speaker_mode: across
  subset: dev-other
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0388590693473815
  seed: 3459
  speaker_mode: within
  subset: dev-other
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.0640353336930275
  seed: 3459
  speaker_mode: within
  subset: test-clean
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.0664104893803596
  seed: 3459
  speaker_mode: across
  subset: test-clean
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0297006517648696
  seed: 3459
  speaker_mode: across
  subset: test-clean
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0201348569244146
  seed: 3459
  speaker_mode: within
  subset: test-clean
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.0949535891413688
  seed: 3459
  speaker_mode: within
  subset: test-other
- context_mode: any
  granularity: phoneme
  pooling: none
  score: 0.1047180145978927
  seed: 3459
  speaker_mode: across
  subset: test-other
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.067604586482048
  seed: 3459
  speaker_mode: across
  subset: test-other
- context_mode: within
  granularity: phoneme
  pooling: none
  score: 0.0402371063828468
  seed: 3459
  speaker_mode: within
  subset: test-other
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0328
  seed: none
  speaker_mode: within
  subset: dev-clean
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0414
  seed: none
  speaker_mode: across
  subset: dev-clean
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0497
  seed: none
  speaker_mode: within
  subset: dev-other
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0827
  seed: none
  speaker_mode: across
  subset: dev-other
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0315
  seed: none
  speaker_mode: within
  subset: test-clean
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0425
  seed: none
  speaker_mode: across
  subset: test-clean
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0497
  seed: none
  speaker_mode: within
  subset: test-other
- context_mode: within
  granularity: triphone
  pooling: none
  score: 0.0864
  seed: none
  speaker_mode: across
  subset: test-other
submission_date: '2021-03-23T16:23:00+00:00'
submission_id: 20210323162300_fyuya
submitted_by: null
